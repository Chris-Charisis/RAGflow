{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b869fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "from typing import Any, Dict, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2c728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'schema': 1, 'event': 'ingest', 'source': {'bucket': 'pdf-data', 'object': 'mdpi.pdf', 'etag': '17f1dd801c005a1237c7c42b9a9feada'}, 'metadata': {'title': 'Monitoring Mushroom Growth with Machine Learning', 'authors': 'Vasileios Moysiadis, Georgios Kokkonis, Stamatia Bibi, Ioannis Moscholios, Nikolaos Maropoulos, Panagiotis Sarigiannidis', 'keywords': 'mushroom; YOLOv5; Detectron2; machine learning; object detection; instance segmentation', 'abstract': \"Mushrooms contain valuable nutrients, proteins, minerals, and vitamins, and it is suggested to include them in our diet. Many farmers grow mushrooms in restricted environments with specific atmospheric parameters in greenhouses. In addition, recent technologies of the Internet of things intend to give solutions in the agriculture area. In this paper, we evaluate the effectiveness of machine learning for mushroom growth monitoring for the genus Pleurotus. We use YOLOv5 to detect mushrooms' growing stage and indicate those ready to harvest. The results show that it can detect mushrooms in the greenhouse with an F1-score of up to 76.5%. The classification in the final stage of mushroom growth gives an accuracy of up to 70%, which is acceptable considering the complexity of the photos used. In addition, we propose a method for mushroom growth monitoring based on Detectron2. Our method shows that the average growth period of the mushrooms is 5.22 days. Moreover, our method is also adequate to indicate the harvesting day. The evaluation results show that it could improve the time to harvest for 14.04% of the mushrooms.\", 'doi': '10.3390/agriculture13010223'}, 'text': 'Abstract: Mushrooms contain valuable nutrients, proteins, minerals, and vitamins, and it is suggested to include them in our diet. Many farmers grow mushrooms in restricted environments with speciﬁc atmospheric parameters in greenhouses. In addition, recent technologies of the Internet of things intend to give solutions in the agriculture area. In this paper, we evaluate the effectiveness of machine learning for mushroom growth monitoring for the genus Pleurotus. We use YOLOv5 to detect mushrooms’ growing stage and indicate those ready to harvest. The results show that it can detect mushrooms in the greenhouse with an F1-score of up to 76.5%. The classiﬁcation in the ﬁnal stage of mushroom growth gives an accuracy of up to 70%, which is acceptable considering the complexity of the photos used. In addition, we propose a method for mushroom growth monitoring based on Detectron2. Our method shows that the average growth period of the mushrooms is 5.22 days. Moreover, our method is also adequate to indicate the harvesting day. The evaluation results show that it could improve the time to harvest for 14.04% of the mushrooms. ===introduction=== Citation: Moysiadis, V.; Kokkonis, G.; Bibi, S.; Moscholios, I.; Maropoulos, N.; Sarigiannidis, P. Monitoring Mushroom Growth with Machine Learning. Agriculture 2023, 13, 223. https://doi.org/10.3390/ agriculture13010223 Copyright: © 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). Mushrooms are a rich source of nutrients, proteins, minerals, and vitamins (B, C, and D) [1]. More than 200 species of edible mushrooms are used as ingredients in traditional foods around the world [2], but only 35 species are cultivated in greenhouses in restricted environments [3]. The mushrooms of the genus Pleurotus are in second place worldwide in the industry. Therefore, improving the production line will have a major impact on the economy of the speciﬁc domain. The authors in [4] presented a comprehensive review of the factors affecting the mushrooms of the genus Pleurotus spp. In addition, many research efforts have focused on solutions based on modern technologies from the Internet of things, aiming to transform traditional farming into the new era called smart farming [5]. This transition brings many applications in various agriculture tasks aiming to increase production, reduce cost production, reduce chemical inputs, and reduce labour effort. Along with other technologies, machine learning is highly used in tasks such as yield prediction, disease detection, weed recognition, and fruit recognition. More speciﬁcally, many research efforts try to classify and detect the speciﬁc location of different objects in images. For example, a robotic mechanism is suitable for weeding if it has the ability to recognise weeds from vegetation [6]. The detection of fruits with machine learning is also valuable in robotic mechanisms [7–9]. Disease detection has already seen numerous research efforts evaluating the ability to detect various diseases in various crops [10–12]. Moreover, insect detection is another important task in cultivation. Thus, many research efforts exist in this area [13,14]. Furthermore, machine learning is already used in research efforts in mushroom culti- vation or wild mushroom hunting, aiming to give solutions to various tasks. The authors in [15] gave a comprehensive review in this area. One of the main tasks based on machine learning focuses on species recognition. For example, the authors in [16] provided a method to distinguish edible mushrooms from poisonous ones. In addition, the authors in [17] suggested a method for species recognition with a mobile smart device. Another research effort on mushroom species classiﬁcation is presented in [18]. Furthermore, a few research efforts have used object detection algorithms in order to localise and classify mushrooms in a photo. The authors in [19] developed a system that used object detection to localise and recognise mushrooms that were ready to harvest. In [20], the authors evaluated the poten- tial of mushroom hunting using a custom-made unmanned aerial vehicle (UAV). Finally, the authors in [21] presented an automatic robotic mechanism for mushroom harvesting using machine learning for detection. Object detection algorithms have gained signiﬁcant momentum in recent years due to the success of deep learning [22], a speciﬁc area of machine learning. As a result, many object detection algorithms have been available in recent years that can detect and provide the exact location of an object in an image or video. In addition, some of them are also capable of providing a corresponding mask of the object. This category is called instance segmentation. Detectron2 [23] is the most popular in this category, while some other variants, such as Mask Scoring RCNN [24], try to return better results of the provided masks. In addition, YOLOv5 [25] is one of the most popular object detection algorithms, delivering accurate and rapid results. With recent technologies, a sufﬁcient number of research efforts are available for mush- room cultivation. Most of them propose controlling growing conditions in the greenhouse and keeping them within speciﬁc boundaries [26,27]. However, only a few of the research efforts go a step over and use more advanced mushroom cultivation methods. Machine learning promise to provide efﬁcient methods in this direction. Thus, we should use it to help farmers in their everyday activities and support their decisions. Until now, only a few research efforts have used machine learning in the mushroom industry. As described before, most of them deal with mushroom species classiﬁcation, and only a few try to give solutions for usable tasks such as mushroom harvesting in the greenhouse. Moreover, none of the existing methods for mushroom harvesting utilises the genus Pleurotus. Our work uses machine learning not only to detect the mushrooms in the greenhouse but also to classify them in three different stages depending on their growth status and predict when they are ready to harvest. For this purpose, we use the YOLOv5 [25] object detection algorithm and evaluate its accuracy on mushroom detection with different con- ﬁgurations of hyperparameters. The evaluation results show that YOLOv5 can detect and classify mushrooms with an F1-score of up to 76.5% and detect the ﬁnal stage of mushroom growth with an accuracy of up to 70%. Furthermore, in the second part of our research, we use Detectron2 [23] to extract a corresponding mask for each mushroom and use it to calculate its size and monitor the growth rate. The results show that they follow a linear growth rate, and it is also possible to predict the harvesting day based on images on previous days. In addition, the method can improve the harvesting time for 14.04% of the mushrooms. Our work could be a valuable method for mushroom detection on greenhouses with practical applications. For example, it could be used for yield prediction, where ground cameras can observe the greenhouse and predict the yield a few days before the mushrooms are ready to harvest. Another possible application could be on robotics mechanisms that can detect the exact crown of the mushrooms in order to harvest them with no damage. This work is part of our decision support system described in our previous work [28], where we presented a system architecture that covered mushroom cultivation in a greenhouse and the aggregation of useful information for wild mushroom hunting. The rest of this paper is as follows. In Section 2, we give the most relevant research efforts for mushroom detection using machine learning or image processing. Section 3 brieﬂy discusses the basic concepts of object detection and instance segmentation and provides the essential features of YOLOv5 and Detectron2. Section 4 analyses the method- ology we follow in this research. Next, Section 5 presents the evaluation results between different conﬁgurations of YOLOv5 and its effectiveness on mushroom classiﬁcation at different stages of mushroom growth. In addition, we provide the results from Detectron2 on mushroom growth monitoring. Section 6 discusses the effectiveness of the proposed methods, their limitations and possible impact on future applications. Finally, Section 7 concludes this paper. ===related work=== In this section, we brieﬂy present similar research efforts in the area of smart farming, particularly in image processing or machine learning for image segmentation or object recognition on mushroom cultivation. A harvesting robot for oyster mushrooms was proposed in [21]. The development system used an improved SSD algorithm to detect mushrooms ready to harvest. The algorithm used RGB images and point clouds collected by an Intel RealSense D435i camera. The evaluation results showed an accuracy of mushroom recognition of up to 95.0% and a harvesting success rate of up to 86.8%. The average harvesting time for a single mushroom was 8.85 s. The authors in [19] provided a measurement monitoring system to observe mushroom growth in a greenhouse. The proposed system used YOLOv3 for mushroom detection and an additional localisation method to improve the position of the detected mushroom in order to distinguish the same mushroom in different captured photos. Moreover, the system was able to estimate the harvesting time. In [16], the authors presented a mushroom farm automation to classify toxic mush- rooms. The proposed system adopted machine learning to distinguish edible mushrooms from poisonous mushrooms. The utilised model was a combination of six different clas- siﬁers that worked together and reach an accuracy of 100%. More speciﬁcally, the en- semble model used the following classiﬁers: decision tree (DT), logistic regression (LR), K-nearest neighbours (KNN), support vector machine (SVM), naive Bayes (NB), and ran- dom forest (RF). Moreover, the manuscript introduced an architectural design for smart mushroom farming. The authors in [29] presented an automatic sorting system for fresh, white button mushrooms. Apart from the automatic mechanism, they proposed an image processing algorithm to collect button mushrooms. The algorithm eliminated the shadow and petiole on the image and determined the pileus diameter of the mushrooms. Experimental results showed that compared to manual grading speed, their approach was improved by 38.86%, and the accuracy was improved by 6.84%. ===background=== In this section, we brieﬂy presenting the main characteristics of YOLOv5 [25] and Detectron2 [23]. Both of them are suitable for object detection in images or videos. Moreover, they both belong to object detection algorithms based on convolutional neural networks. Thus, they return the class and the position of the detected object. Furthermore, Detectron2 is also an instance segmentation algorithm able to return a corresponding mask that deﬁnes the area of the object. YOLOv5 [25] comes from You Only Look Once and claims to be one of the fastest object detection algorithms. In fact, it is one of the best-known object detection algorithms due to its speed and accuracy. It is divided into three components, namely, a backbone, neck and head, as all single-stage detectors. In particular, the ﬁrst component contains a backbone network that extracts rich feature representations from images. The second component consists of the model neck responsible for extracting feature pyramids that help to recognise objects of different sizes and scales. The ﬁnal component is the model head that applies anchor boxes on feature maps. In addition, it is responsible for rendering the predicted class, the scores of the predicted objects, and the bounding boxes. Detectron2 [23] is considered state-of-the-art in instance segmentation. It is the succes- sor of Mask RCNN [30], which is built on top of Faster RCNN. Thus, apart from providing The architecture of Mask RCNN is divided into three stages. In the ﬁrst stage, a regional proposal network (RPN) is responsible for returning all regions of possible areas with detected objects. Mask RCNN uses ResNet50 or ResNet101 with FPN support as a backbone network in this stage. In the second stage, a classiﬁer is responsible for evaluating the region proposals derived from the ﬁrst stage and providing the predictions with bounding boxes for each detected object. Finally, the third stage provides the corresponding masks of the detected objects. ===materials and methods=== This section presents the methodology we followed in our research. First, we describe the image acquisition from mushrooms in the greenhouse and the annotation process. Second, we give the basic conﬁguration of YOLOv5 for mushroom classiﬁcation in three different growing stages. Moreover, we provide the architecture we used for mushroom growth monitoring. Data collection took place in a greenhouse near the city of Grevena, Greece. We collected multiple images in a greenhouse, each containing one or multiple mushrooms. We captured only images that had mushrooms mainly in the foreground and not in the background because they reduced the detection accuracy. Finally, we obtained 1128 images with one or more mushrooms. We annotated them into three different classes (Stage1, Stage2, Stage3) depending on the growing stage. In more detail, the ﬁrst stage corresponded to the ﬁrst days of appearance, when mushrooms were too small and they had just started to form their shape. The second class corresponded to mushrooms that had already formed their shape and continued to grow. Some of them were obviously small, while others were big enough but not ready to harvest. The third class contained mushrooms that were ready to harvest, which could be manually indicated from two parameters. More speciﬁcally, when the edges of their caps started to become ﬂat or slightly uprolled, it was an indication of their ﬁnal stage of growth [31]. Figure 1 shows some examples of mushrooms belonging to the three different stages. Many of the collected photos in the greenhouse contained multiple mushrooms, making the annotation complex and the recognition even more challenging, especially when the mushrooms were at the ﬁnal stage and many of them were overlapping. Figure 2 presents two of those photos as an example. Figure 2. Pictures from the greenhouse with multiple mushrooms. Finally, 4271 mushrooms were annotated and classiﬁed into three different classes (Stage1, Stage2, Stage3) as described before. Moreover, 1130 of them belonged to the ﬁrst growing stage, 1845 to the second growing stage, and 1296 of them to the third growing stage. In addition, the 1128 annotated photos were divided into 784 for the training dataset and 344 for the validation dataset. For the second part of our research, we collected a different set of photos. In particular, the data acquisition for the growing rate of mushrooms was made in 33 different mushroom substrate grow bags. We captured a photo for each one on seven different days between 16 August 2022 and 24 August 2022. Finally, we collected 231 different photos in total. The distance between the camera and the substrate grow bag was one meter above. Figure 3 shows an example of three captured photos. We did not annotate these photos, but we used the trained models from Detectron2 to indicate the size of each mushroom. In our evaluation for mushroom growth monitoring, we used only the mushrooms that were on top of the substrate grow bag. For training Detectron2 for instance segmentation, we annotated another two datasets. The ﬁrst one contained annotations for substrate grow bags, and the second one contained annotations for mushrooms. The annotation process for the image segmentation is a time- consuming task since we had to annotate precisely the objects with polygons. For that reason, we chose to annotate only photos that contained a small number of mushrooms to simplify the annotation. Finally, we annotated 453 mushrooms and divided them into 358 for the training dataset and 95 for the validation dataset. In addition, for the detection of substrate grow bags, we annotated 200 photos and divided them into 150 for the training dataset and 50 for the validation dataset. For mushroom detection and classiﬁcation in the three different classes as described previously, we used YOLOv5. First, we trained our models with the default hyperparame- ters using the pretrained models YOLOv5s and YOLOv5l with an image size of 640 pixels and batch sizes of 2, 4, and 8. To achieve better results, we used the provided method integrated with YOLOv5 for hyperparameter optimisation called hyperparameter evolution. It is a genetic algorithm to ﬁnd the best set of hyperparameters for the speciﬁc dataset. The evolving procedure starts from the default hyperparameters or other user-deﬁned values, if available, and tries to improve a ﬁtness function in every loop. The default ﬁtness function is a weighted combination of mAP@0.5 with 10% contribution and mAP@0.5:0.95 with 90% contribution. In every evolving loop, the primary genetic operator is the mutation. The proposed combination for the mutation uses 80% probability and a 0.04 variance to calculate the next combination of hyperparameters based on the best parents from previous generations. We used the hyperparameter evolution approach for the two pretrained models, YOLOv5s and YOLOv5l, and trained them for 600 generations with an image size of 640 pixels and a batch size of 4. Table 1 shows the predicted sets of hyperparameters for each pretrained model. Table 1. Optimised hyperparameters after evolving for YOLOv5s and YOLOv5l. Figure 4 shows a graphical representation for YOLOv5l with each hyperparameter displayed in a different subplot. Each subplot presents all values from all generations for the speciﬁc hyperparameter. The horizontal axis corresponds to the value of the hyperparameter, and the vertical value corresponds to the calculated ﬁtness. The yellow areas indicate a high concentration of values. Subplots with vertical distributions indicate that the speciﬁc hyperparameter was disabled and did not mutate. Figure 4. Graphical representation for the calculated values of all hyperparameters, after evolving for 600 generations for YOLOv5l. The mAP@0.5 metric corresponds to the average precision over a threshold of 0.5 for the object detection. In addition, the mAP@0.5:0.95 denotes the average precision with a threshold between 0.5 and 0.95. The precision metric indicates the correctly identiﬁed trees divided by the total number of detected trees and is given by Equation (1). The recall metric indicates the falsely identi- ﬁed trees divided by the total number of actual existing trees and is given by Equation (2). where TP (true positive) is the number of correctly detected objects in the class, FP (false positive) is the number of falsely detected objects in the speciﬁc class, and FN (false negative) is the number of objects that are not detected in the speciﬁc class. For mushroom growth monitoring, we used two different stages for the object detec- tion. The ﬁrst one was responsible for detecting the substrate grow bag, and the second one was responsible for detecting the mushrooms in a photo. Figure 5 illustrates the main components of the architecture of the proposed method. More speciﬁcally, in the stage of the substrate grow bag detection, we use the trained model with Detectron2. We only obtained the bounding box of the detected substrate grow bag and not the mask. After the detection of the substrate grow bag, we used the corresponding bounding box and divided it into six equal rectangles (2 × 3). Figure 6a illustrates the bounding box (green) and the lines (red) that divide the bounding box in order to distinguish the detected mushrooms. Moreover, we used the trained model with Detectron2 to detect the mushrooms in the photo and return the corresponding masks. Figure 6b displays the detected mush- rooms. Next, we performed a resize operation on the bounding box and the masks to achieve normalisation for calculating the size in all photos of the same substrate grow bag. Furthermore, the detected mushrooms were distinguished in one of the rectangles with the following rules. A mushroom should have more than 50% in the speciﬁc rectangle. Only one mushroom can belong in one rectangle, and each mushroom belongs only in one rectangle. In any case, the selected mushroom was the one that covered the most area in the rectangle. Finally, Figure 6c illustrates the divided bounding box with the corresponding masks of all detected masks. The size of the provided mask could indicate the size of the mushroom. After distinguishing each mushroom, we calculated the coverage based on the pixels of the provided mask and used it for mushroom growth monitoring. ===results=== In the following subsections, we evaluate the results of the mushroom classiﬁcation for different growth stages. Next, we provide the results of mushroom monitoring for their growth rate. In this subsection, we compared different conﬁgurations of YOLOv5 for mushroom detection and classiﬁcation in three classes, as described previously. We used the metrics mAP@0.5, mAP@0.5:0.95, precision, recall, and F1-score to compare the accuracy of each trained model. Table 2 shows a comparison of all conﬁgurations. More speciﬁcally, we use the pretrained models YOLOv5s and YOLOv5l with the default (D) conﬁguration for the hyperparameters and with the conﬁguration derived from evolution (E). Moreover, our conﬁgurations used an image size of 640 pixels and three different values for the batch size (2, 4, 8). Table 2. Evaluation results for mAP@0.5 and mAP@0.5:0.95 for different hyperparameter conﬁgura- tions (HC), for default values (D) and optimised values with evolution (E). It seems that there was not much difference in all conﬁgurations. Based on mAP@50 and mAP@50:95, conﬁgurations with hyperparameters derived from evolution showed a slightly better performance. Moreover, conﬁgurations based on YOLOv5l returned a slightly better performance. The model based on YOLOv5l with hyperparameters de- rived from evolution and with a batch size of eight achieved the best performance. More speciﬁcally, it reached an mAP@0.5 of 0.79353 and an mAP@0.5:0.95 of 0.54582. Another type of evaluation was based on the F1-Score metric. Table 3 shows the comparison of all conﬁgurations based on the best occurrence of the F1-Score. Better performance on this metric was achieved by the same conﬁguration with YOLOv5l, hy- perparameters derived from evolution, and a batch size of eight, as it reached an F1-Score of 76.65%. Table 3. Evaluation results for the F1-Score for different hyperparameter conﬁgurations (HC), for default values (D) and optimised values with evolution (E). Figure 7 shows the curve of the F1-Score for the best conﬁguration. The horizontal axis corresponds to the conﬁdence of the detected mushrooms, while the vertical axis corresponds to the F1-score. In this example, the maximum F1-score had a conﬁdence of 0.566. In addition, the graph shows that the class “Stage1” had a better accuracy. This was expected since all photos with mushrooms belonging to “Stage1” were simpler, as mushrooms were small and not overlapping. To notice the detection accuracy for each class more clearly, Figure 8 presents the confusion matrix for the same conﬁguration. It shows that class “Stage1” had an accuracy of 82%, class “Stage2” had an accuracy of 71%, and “Stage3” had an accuracy of 70%. Although this does not seem perfect, we can say that it was good enough depending on the complexity of the photos with mushrooms in “Stage2” and “Stage3”. Figure 9 shows an example of the detection where only mushrooms from classes Stage2 and Stage3 exist. The green bounding box indicates that the mushroom is ready to harvest (Stage3). The blue bounding box indicates that the mushroom is not yet ready to harvest (Stage2). We marked in magenta the mushrooms that were detected in the wrong class. Moreover, we marked in red the mushrooms that were not detected. It seems from the photo that the mushrooms in the foreground were detected in the right class, while some errors occurred in mushrooms that were in the background or partially displayed. After the procedure described previously, we obtained 575 different masks from 114 different mushrooms in the greenhouse. All masks were distinguished for different mushrooms depending on the number of the substrate grow bag and the position on it. When the mask corresponded to the ﬁrst day of appearance of each mushroom, it was marked as day one. The corresponding masks of the next days of the same mushroom were numbered as well in the same way. Figure 10 shows the growth rate of four different mushrooms. Each mushroom is indicated with a different colour. The horizontal axis of the graph contains the number of the growth day, starting from number one for the ﬁrst day of the mushroom’s appearance. The vertical axis indicates the size of the mushroom counted in pixels. Moreover, we calculated the maximum size of each mushroom based on all photos before it was harvested. We set its size as 100%, and we calculated the size of the same mushroom for all other days accordingly. Figure 11 shows the growth rate of the same mushrooms but as a percentage of the maximum size of each mushroom. Both graphs show that the three mushrooms (M1, M2, M3) followed an almost linear growth rate. The fourth mushroom (M4) also followed a linear growth rate for the ﬁrst four days, but from the ﬁfth day, it seemed not to change in size. In fact, on the ﬁfth day, that speciﬁc mushroom should have been harvested, but it was accidentally forgotten. Figure 12 shows the captured photos of this mushroom (M4), from which it seemed that the mushroom was ready to harvest on day ﬁve. Figure 12. Mushroom growth example photos. We manually evaluated the harvesting status of each mushroom and decided that 17 mushrooms could be harvested earlier, which represented 14.04% of the mushrooms. The average difference between the size of the masks from the day identiﬁed as ready to harvest and the previous day was 5.34%. Thus, if we set it as a threshold in our method to provide an alert any time that the difference in the mushroom size was below it, we could inform the farmer of the harvest time. This would lead to an improvement in the quality of the harvested mushrooms as we would avoid overripe mushrooms. In addition, our evaluation showed that the average days for the growth of mushrooms was 5.22. Figure 13 displays the graph with all mushrooms detected for all days. All values are displayed in percentage of the maximum size of the speciﬁc mushroom. The blue marks indicate the detected mushrooms that were not ready to harvest, while the red marks indicate mushrooms that should have been harvested at least one day before. ===discussion=== Our ﬁrst approach for mushroom detection with YOLOv5 could be a valuable part of a decision support system. For example, it may be useful in tasks such as yield prediction or product improvement by indicating those mushrooms that are ready for harvest. Overall, the accuracy of the trained model was considered acceptable regarding the complexity of the photos of the dataset. However, our experience shows that some changes could improve the results. A more precisely selected dataset should improve the detection accuracy of different classes. For example, photos with fewer mushrooms and only in the foreground will give more accurate results. As we discussed in Figure 9, the main drawback in detection accuracy is that many mushrooms are partially visible in the photos, and many of them are in the background, which makes it difﬁcult to detect and classify because their features are not clear. Moreover, a proposed methodology to overcome these obstacles could be to exclude mushrooms that are partially displayed in the picture and to focus only on those clearly displayed. Furthermore, the 600 steps we used for the hyperparameter evolution in this research may not be good enough, and more steps will result in a better detection accuracy. This would lead to better combinations of the hyperparameters, but the hyperparameter evolu- tion procedure is a very time-consuming task, even for a professional graphic processing unit. A better approach to enhance the model’s accuracy is to modify the YOLOv5 algorithm. Thus, the trained model would include the speciﬁc characteristics of the different classes. Our second approach for mushroom growth monitoring could also be helpful for a decision support system to inform farmers about the harvesting time. As a result, an improvement in product quality is expected since mushrooms will be collected at the right time. Furthermore, the precisely detected mask of the mushrooms may support other potential applications. For example, it could be a useful part of a robotic mechanism in order to detect, decide, and collect only those mushrooms that are ready to harvest without damaging them. The method of mushroom growth monitoring with Detectron2 also has some limi- tations. Our method focused only on mushrooms that were on top of the substrate bag, which was placed horizontally in the greenhouse. In fact, only one-third of the mushrooms could be observed with this method since there were also substrate bags positioned verti- cally below the substrate bag we used. In addition, in many cases, the substrate bags are densely placed in the greenhouse, and sometimes, the farmers use more than one layer of substrate grow bags in the greenhouse, making it difﬁcult to capture photos even from those positioned horizontally. Furthermore, getting only one image per day has the disadvantage that the growing days may not be counted precisely in some mushrooms. For example, some mushrooms may appear a few hours after a picture was taken, resulting in a day count less than the actual growth duration. Moreover, some mushrooms may have been harvested just before a picture was taken, also resulting in reduced count days. Therefore, capturing photos of the substrate bags more often could give better results and inform more precisely about the growing rate of the mushroom and the harvesting time. Finally, the results of the mushroom detection are not always precise. In fact, the proposed method has two minor drawbacks. First, some of the mushrooms that were too small could not be detected. In addition, the masks returned from Detectron2 were not always accurate. We believe that both these drawbacks do not have a large impact on the proposed methodology. However, if we need to improve them, we could use a larger dataset with more annotated mushrooms to get better results. ===conclusions=== Object detection and instance segmentation have multiple applications in various domains. In smart farming, it is mainly used in fruit detection, weed detection, or pest detection. In this paper, we evaluated the effectiveness of YOLOv5 and Detectron2 in mushroom detection in a greenhouse. Firstly, we evaluated YOLOv5 on its effectiveness in classifying mushrooms in three growth stages. The results showed that even in complex environments such as a greenhouse with Pleurotus mushrooms, it was possible to identify mushrooms that were ready to harvest. The evaluation results on mushroom detection and classiﬁcation in three different growing stages gave an F1-score of up to 76.5%, and speciﬁcally for the ﬁnal growing stage, an accuracy of up to 70%. Secondly, we proposed a method for mushroom growth monitoring. For that purpose, we used two trained models with Detectron2. The ﬁrst one detected the substrate grow bag, and the second detected the mushrooms and returned the corresponding masks. Experi- mental results showed that the growth rate of Pleurotus mushrooms was linear. Moreover, this procedure made it possible to detect when mushrooms reached their maximum size and were ready to harvest. The evaluation results showed that it was possible to make de- cisions and improve harvesting time for up to 14.04% of the mushrooms in the greenhouse. In addition, the results showed that, on average, Pleurotus mushrooms needed 5.22 days to reach maximum size. Moreover, the proposed methods are suitable to be part of a decision support system to inform farmers about the status of their cultivation. In addition, a future potential application would be a robotic mechanism able to detect and recognise the growth stage of mushrooms before harvesting them. Author Contributions: Conceptualization, V.M. and P.S.; methodology, V.M. and G.K.; software, V.M.; validation, G.K. and P.S.; formal analysis, V.M.; investigation, V.M.; resources, V.M. and N.M.; data curation, V.M. and G.K.; writing—original draft preparation, V.M.; writing—review and editing, S.B. and I.M.; visualization, V.M.; supervision, P.S.; project administration, P.S.; funding acquisition, P.S. All authors have read and agreed to the published version of the manuscript. Funding: This research was cofunded by the European Regional Development Fund of the European Union and Greek national funds through the Operational Program Western Macedonia 2014–2020, under the call “Collaborative and networking actions between research institutions, educational institutions and companies in priority areas of the strategic smart specialization plan of the region”, project “Smart Mushroom fARming with internet of Things—SMART”, project code: DMR-0016521. Data Availability Statement: The data presented in this study are available on request from the corresponding author. The data are not publicly available due to restrictions on privacy. ===references=== Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.'}\n"
     ]
    }
   ],
   "source": [
    "# read json file to variable\n",
    "# with open('../pdf_reader/outputs/mdpi.pdf_processed.json') as f:\n",
    "with open('../pdf_reader/outputs/mdpi.pdf_processed.json') as f:\n",
    "    d = json.load(f)\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b83b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'schema': 1,\n",
       " 'event': 'ingest',\n",
       " 'source': {'bucket': 'pdf-data',\n",
       "  'object': 'mdpi.pdf',\n",
       "  'etag': '17f1dd801c005a1237c7c42b9a9feada'},\n",
       " 'metadata': {'title': 'Monitoring Mushroom Growth with Machine Learning',\n",
       "  'authors': 'Vasileios Moysiadis, Georgios Kokkonis, Stamatia Bibi, Ioannis Moscholios, Nikolaos Maropoulos, Panagiotis Sarigiannidis',\n",
       "  'keywords': 'mushroom; YOLOv5; Detectron2; machine learning; object detection; instance segmentation',\n",
       "  'abstract': \"Mushrooms contain valuable nutrients, proteins, minerals, and vitamins, and it is suggested to include them in our diet. Many farmers grow mushrooms in restricted environments with specific atmospheric parameters in greenhouses. In addition, recent technologies of the Internet of things intend to give solutions in the agriculture area. In this paper, we evaluate the effectiveness of machine learning for mushroom growth monitoring for the genus Pleurotus. We use YOLOv5 to detect mushrooms' growing stage and indicate those ready to harvest. The results show that it can detect mushrooms in the greenhouse with an F1-score of up to 76.5%. The classification in the final stage of mushroom growth gives an accuracy of up to 70%, which is acceptable considering the complexity of the photos used. In addition, we propose a method for mushroom growth monitoring based on Detectron2. Our method shows that the average growth period of the mushrooms is 5.22 days. Moreover, our method is also adequate to indicate the harvesting day. The evaluation results show that it could improve the time to harvest for 14.04% of the mushrooms.\",\n",
       "  'doi': '10.3390/agriculture13010223'},\n",
       " 'text': 'Abstract: Mushrooms contain valuable nutrients, proteins, minerals, and vitamins, and it is suggested to include them in our diet. Many farmers grow mushrooms in restricted environments with speciﬁc atmospheric parameters in greenhouses. In addition, recent technologies of the Internet of things intend to give solutions in the agriculture area. In this paper, we evaluate the effectiveness of machine learning for mushroom growth monitoring for the genus Pleurotus. We use YOLOv5 to detect mushrooms’ growing stage and indicate those ready to harvest. The results show that it can detect mushrooms in the greenhouse with an F1-score of up to 76.5%. The classiﬁcation in the ﬁnal stage of mushroom growth gives an accuracy of up to 70%, which is acceptable considering the complexity of the photos used. In addition, we propose a method for mushroom growth monitoring based on Detectron2. Our method shows that the average growth period of the mushrooms is 5.22 days. Moreover, our method is also adequate to indicate the harvesting day. The evaluation results show that it could improve the time to harvest for 14.04% of the mushrooms. ===introduction=== Citation: Moysiadis, V.; Kokkonis, G.; Bibi, S.; Moscholios, I.; Maropoulos, N.; Sarigiannidis, P. Monitoring Mushroom Growth with Machine Learning. Agriculture 2023, 13, 223. https://doi.org/10.3390/ agriculture13010223 Copyright: © 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). Mushrooms are a rich source of nutrients, proteins, minerals, and vitamins (B, C, and D) [1]. More than 200 species of edible mushrooms are used as ingredients in traditional foods around the world [2], but only 35 species are cultivated in greenhouses in restricted environments [3]. The mushrooms of the genus Pleurotus are in second place worldwide in the industry. Therefore, improving the production line will have a major impact on the economy of the speciﬁc domain. The authors in [4] presented a comprehensive review of the factors affecting the mushrooms of the genus Pleurotus spp. In addition, many research efforts have focused on solutions based on modern technologies from the Internet of things, aiming to transform traditional farming into the new era called smart farming [5]. This transition brings many applications in various agriculture tasks aiming to increase production, reduce cost production, reduce chemical inputs, and reduce labour effort. Along with other technologies, machine learning is highly used in tasks such as yield prediction, disease detection, weed recognition, and fruit recognition. More speciﬁcally, many research efforts try to classify and detect the speciﬁc location of different objects in images. For example, a robotic mechanism is suitable for weeding if it has the ability to recognise weeds from vegetation [6]. The detection of fruits with machine learning is also valuable in robotic mechanisms [7–9]. Disease detection has already seen numerous research efforts evaluating the ability to detect various diseases in various crops [10–12]. Moreover, insect detection is another important task in cultivation. Thus, many research efforts exist in this area [13,14]. Furthermore, machine learning is already used in research efforts in mushroom culti- vation or wild mushroom hunting, aiming to give solutions to various tasks. The authors in [15] gave a comprehensive review in this area. One of the main tasks based on machine learning focuses on species recognition. For example, the authors in [16] provided a method to distinguish edible mushrooms from poisonous ones. In addition, the authors in [17] suggested a method for species recognition with a mobile smart device. Another research effort on mushroom species classiﬁcation is presented in [18]. Furthermore, a few research efforts have used object detection algorithms in order to localise and classify mushrooms in a photo. The authors in [19] developed a system that used object detection to localise and recognise mushrooms that were ready to harvest. In [20], the authors evaluated the poten- tial of mushroom hunting using a custom-made unmanned aerial vehicle (UAV). Finally, the authors in [21] presented an automatic robotic mechanism for mushroom harvesting using machine learning for detection. Object detection algorithms have gained signiﬁcant momentum in recent years due to the success of deep learning [22], a speciﬁc area of machine learning. As a result, many object detection algorithms have been available in recent years that can detect and provide the exact location of an object in an image or video. In addition, some of them are also capable of providing a corresponding mask of the object. This category is called instance segmentation. Detectron2 [23] is the most popular in this category, while some other variants, such as Mask Scoring RCNN [24], try to return better results of the provided masks. In addition, YOLOv5 [25] is one of the most popular object detection algorithms, delivering accurate and rapid results. With recent technologies, a sufﬁcient number of research efforts are available for mush- room cultivation. Most of them propose controlling growing conditions in the greenhouse and keeping them within speciﬁc boundaries [26,27]. However, only a few of the research efforts go a step over and use more advanced mushroom cultivation methods. Machine learning promise to provide efﬁcient methods in this direction. Thus, we should use it to help farmers in their everyday activities and support their decisions. Until now, only a few research efforts have used machine learning in the mushroom industry. As described before, most of them deal with mushroom species classiﬁcation, and only a few try to give solutions for usable tasks such as mushroom harvesting in the greenhouse. Moreover, none of the existing methods for mushroom harvesting utilises the genus Pleurotus. Our work uses machine learning not only to detect the mushrooms in the greenhouse but also to classify them in three different stages depending on their growth status and predict when they are ready to harvest. For this purpose, we use the YOLOv5 [25] object detection algorithm and evaluate its accuracy on mushroom detection with different con- ﬁgurations of hyperparameters. The evaluation results show that YOLOv5 can detect and classify mushrooms with an F1-score of up to 76.5% and detect the ﬁnal stage of mushroom growth with an accuracy of up to 70%. Furthermore, in the second part of our research, we use Detectron2 [23] to extract a corresponding mask for each mushroom and use it to calculate its size and monitor the growth rate. The results show that they follow a linear growth rate, and it is also possible to predict the harvesting day based on images on previous days. In addition, the method can improve the harvesting time for 14.04% of the mushrooms. Our work could be a valuable method for mushroom detection on greenhouses with practical applications. For example, it could be used for yield prediction, where ground cameras can observe the greenhouse and predict the yield a few days before the mushrooms are ready to harvest. Another possible application could be on robotics mechanisms that can detect the exact crown of the mushrooms in order to harvest them with no damage. This work is part of our decision support system described in our previous work [28], where we presented a system architecture that covered mushroom cultivation in a greenhouse and the aggregation of useful information for wild mushroom hunting. The rest of this paper is as follows. In Section 2, we give the most relevant research efforts for mushroom detection using machine learning or image processing. Section 3 brieﬂy discusses the basic concepts of object detection and instance segmentation and provides the essential features of YOLOv5 and Detectron2. Section 4 analyses the method- ology we follow in this research. Next, Section 5 presents the evaluation results between different conﬁgurations of YOLOv5 and its effectiveness on mushroom classiﬁcation at different stages of mushroom growth. In addition, we provide the results from Detectron2 on mushroom growth monitoring. Section 6 discusses the effectiveness of the proposed methods, their limitations and possible impact on future applications. Finally, Section 7 concludes this paper. ===related work=== In this section, we brieﬂy present similar research efforts in the area of smart farming, particularly in image processing or machine learning for image segmentation or object recognition on mushroom cultivation. A harvesting robot for oyster mushrooms was proposed in [21]. The development system used an improved SSD algorithm to detect mushrooms ready to harvest. The algorithm used RGB images and point clouds collected by an Intel RealSense D435i camera. The evaluation results showed an accuracy of mushroom recognition of up to 95.0% and a harvesting success rate of up to 86.8%. The average harvesting time for a single mushroom was 8.85 s. The authors in [19] provided a measurement monitoring system to observe mushroom growth in a greenhouse. The proposed system used YOLOv3 for mushroom detection and an additional localisation method to improve the position of the detected mushroom in order to distinguish the same mushroom in different captured photos. Moreover, the system was able to estimate the harvesting time. In [16], the authors presented a mushroom farm automation to classify toxic mush- rooms. The proposed system adopted machine learning to distinguish edible mushrooms from poisonous mushrooms. The utilised model was a combination of six different clas- siﬁers that worked together and reach an accuracy of 100%. More speciﬁcally, the en- semble model used the following classiﬁers: decision tree (DT), logistic regression (LR), K-nearest neighbours (KNN), support vector machine (SVM), naive Bayes (NB), and ran- dom forest (RF). Moreover, the manuscript introduced an architectural design for smart mushroom farming. The authors in [29] presented an automatic sorting system for fresh, white button mushrooms. Apart from the automatic mechanism, they proposed an image processing algorithm to collect button mushrooms. The algorithm eliminated the shadow and petiole on the image and determined the pileus diameter of the mushrooms. Experimental results showed that compared to manual grading speed, their approach was improved by 38.86%, and the accuracy was improved by 6.84%. ===background=== In this section, we brieﬂy presenting the main characteristics of YOLOv5 [25] and Detectron2 [23]. Both of them are suitable for object detection in images or videos. Moreover, they both belong to object detection algorithms based on convolutional neural networks. Thus, they return the class and the position of the detected object. Furthermore, Detectron2 is also an instance segmentation algorithm able to return a corresponding mask that deﬁnes the area of the object. YOLOv5 [25] comes from You Only Look Once and claims to be one of the fastest object detection algorithms. In fact, it is one of the best-known object detection algorithms due to its speed and accuracy. It is divided into three components, namely, a backbone, neck and head, as all single-stage detectors. In particular, the ﬁrst component contains a backbone network that extracts rich feature representations from images. The second component consists of the model neck responsible for extracting feature pyramids that help to recognise objects of different sizes and scales. The ﬁnal component is the model head that applies anchor boxes on feature maps. In addition, it is responsible for rendering the predicted class, the scores of the predicted objects, and the bounding boxes. Detectron2 [23] is considered state-of-the-art in instance segmentation. It is the succes- sor of Mask RCNN [30], which is built on top of Faster RCNN. Thus, apart from providing The architecture of Mask RCNN is divided into three stages. In the ﬁrst stage, a regional proposal network (RPN) is responsible for returning all regions of possible areas with detected objects. Mask RCNN uses ResNet50 or ResNet101 with FPN support as a backbone network in this stage. In the second stage, a classiﬁer is responsible for evaluating the region proposals derived from the ﬁrst stage and providing the predictions with bounding boxes for each detected object. Finally, the third stage provides the corresponding masks of the detected objects. ===materials and methods=== This section presents the methodology we followed in our research. First, we describe the image acquisition from mushrooms in the greenhouse and the annotation process. Second, we give the basic conﬁguration of YOLOv5 for mushroom classiﬁcation in three different growing stages. Moreover, we provide the architecture we used for mushroom growth monitoring. Data collection took place in a greenhouse near the city of Grevena, Greece. We collected multiple images in a greenhouse, each containing one or multiple mushrooms. We captured only images that had mushrooms mainly in the foreground and not in the background because they reduced the detection accuracy. Finally, we obtained 1128 images with one or more mushrooms. We annotated them into three different classes (Stage1, Stage2, Stage3) depending on the growing stage. In more detail, the ﬁrst stage corresponded to the ﬁrst days of appearance, when mushrooms were too small and they had just started to form their shape. The second class corresponded to mushrooms that had already formed their shape and continued to grow. Some of them were obviously small, while others were big enough but not ready to harvest. The third class contained mushrooms that were ready to harvest, which could be manually indicated from two parameters. More speciﬁcally, when the edges of their caps started to become ﬂat or slightly uprolled, it was an indication of their ﬁnal stage of growth [31]. Figure 1 shows some examples of mushrooms belonging to the three different stages. Many of the collected photos in the greenhouse contained multiple mushrooms, making the annotation complex and the recognition even more challenging, especially when the mushrooms were at the ﬁnal stage and many of them were overlapping. Figure 2 presents two of those photos as an example. Figure 2. Pictures from the greenhouse with multiple mushrooms. Finally, 4271 mushrooms were annotated and classiﬁed into three different classes (Stage1, Stage2, Stage3) as described before. Moreover, 1130 of them belonged to the ﬁrst growing stage, 1845 to the second growing stage, and 1296 of them to the third growing stage. In addition, the 1128 annotated photos were divided into 784 for the training dataset and 344 for the validation dataset. For the second part of our research, we collected a different set of photos. In particular, the data acquisition for the growing rate of mushrooms was made in 33 different mushroom substrate grow bags. We captured a photo for each one on seven different days between 16 August 2022 and 24 August 2022. Finally, we collected 231 different photos in total. The distance between the camera and the substrate grow bag was one meter above. Figure 3 shows an example of three captured photos. We did not annotate these photos, but we used the trained models from Detectron2 to indicate the size of each mushroom. In our evaluation for mushroom growth monitoring, we used only the mushrooms that were on top of the substrate grow bag. For training Detectron2 for instance segmentation, we annotated another two datasets. The ﬁrst one contained annotations for substrate grow bags, and the second one contained annotations for mushrooms. The annotation process for the image segmentation is a time- consuming task since we had to annotate precisely the objects with polygons. For that reason, we chose to annotate only photos that contained a small number of mushrooms to simplify the annotation. Finally, we annotated 453 mushrooms and divided them into 358 for the training dataset and 95 for the validation dataset. In addition, for the detection of substrate grow bags, we annotated 200 photos and divided them into 150 for the training dataset and 50 for the validation dataset. For mushroom detection and classiﬁcation in the three different classes as described previously, we used YOLOv5. First, we trained our models with the default hyperparame- ters using the pretrained models YOLOv5s and YOLOv5l with an image size of 640 pixels and batch sizes of 2, 4, and 8. To achieve better results, we used the provided method integrated with YOLOv5 for hyperparameter optimisation called hyperparameter evolution. It is a genetic algorithm to ﬁnd the best set of hyperparameters for the speciﬁc dataset. The evolving procedure starts from the default hyperparameters or other user-deﬁned values, if available, and tries to improve a ﬁtness function in every loop. The default ﬁtness function is a weighted combination of mAP@0.5 with 10% contribution and mAP@0.5:0.95 with 90% contribution. In every evolving loop, the primary genetic operator is the mutation. The proposed combination for the mutation uses 80% probability and a 0.04 variance to calculate the next combination of hyperparameters based on the best parents from previous generations. We used the hyperparameter evolution approach for the two pretrained models, YOLOv5s and YOLOv5l, and trained them for 600 generations with an image size of 640 pixels and a batch size of 4. Table 1 shows the predicted sets of hyperparameters for each pretrained model. Table 1. Optimised hyperparameters after evolving for YOLOv5s and YOLOv5l. Figure 4 shows a graphical representation for YOLOv5l with each hyperparameter displayed in a different subplot. Each subplot presents all values from all generations for the speciﬁc hyperparameter. The horizontal axis corresponds to the value of the hyperparameter, and the vertical value corresponds to the calculated ﬁtness. The yellow areas indicate a high concentration of values. Subplots with vertical distributions indicate that the speciﬁc hyperparameter was disabled and did not mutate. Figure 4. Graphical representation for the calculated values of all hyperparameters, after evolving for 600 generations for YOLOv5l. The mAP@0.5 metric corresponds to the average precision over a threshold of 0.5 for the object detection. In addition, the mAP@0.5:0.95 denotes the average precision with a threshold between 0.5 and 0.95. The precision metric indicates the correctly identiﬁed trees divided by the total number of detected trees and is given by Equation (1). The recall metric indicates the falsely identi- ﬁed trees divided by the total number of actual existing trees and is given by Equation (2). where TP (true positive) is the number of correctly detected objects in the class, FP (false positive) is the number of falsely detected objects in the speciﬁc class, and FN (false negative) is the number of objects that are not detected in the speciﬁc class. For mushroom growth monitoring, we used two different stages for the object detec- tion. The ﬁrst one was responsible for detecting the substrate grow bag, and the second one was responsible for detecting the mushrooms in a photo. Figure 5 illustrates the main components of the architecture of the proposed method. More speciﬁcally, in the stage of the substrate grow bag detection, we use the trained model with Detectron2. We only obtained the bounding box of the detected substrate grow bag and not the mask. After the detection of the substrate grow bag, we used the corresponding bounding box and divided it into six equal rectangles (2 × 3). Figure 6a illustrates the bounding box (green) and the lines (red) that divide the bounding box in order to distinguish the detected mushrooms. Moreover, we used the trained model with Detectron2 to detect the mushrooms in the photo and return the corresponding masks. Figure 6b displays the detected mush- rooms. Next, we performed a resize operation on the bounding box and the masks to achieve normalisation for calculating the size in all photos of the same substrate grow bag. Furthermore, the detected mushrooms were distinguished in one of the rectangles with the following rules. A mushroom should have more than 50% in the speciﬁc rectangle. Only one mushroom can belong in one rectangle, and each mushroom belongs only in one rectangle. In any case, the selected mushroom was the one that covered the most area in the rectangle. Finally, Figure 6c illustrates the divided bounding box with the corresponding masks of all detected masks. The size of the provided mask could indicate the size of the mushroom. After distinguishing each mushroom, we calculated the coverage based on the pixels of the provided mask and used it for mushroom growth monitoring. ===results=== In the following subsections, we evaluate the results of the mushroom classiﬁcation for different growth stages. Next, we provide the results of mushroom monitoring for their growth rate. In this subsection, we compared different conﬁgurations of YOLOv5 for mushroom detection and classiﬁcation in three classes, as described previously. We used the metrics mAP@0.5, mAP@0.5:0.95, precision, recall, and F1-score to compare the accuracy of each trained model. Table 2 shows a comparison of all conﬁgurations. More speciﬁcally, we use the pretrained models YOLOv5s and YOLOv5l with the default (D) conﬁguration for the hyperparameters and with the conﬁguration derived from evolution (E). Moreover, our conﬁgurations used an image size of 640 pixels and three different values for the batch size (2, 4, 8). Table 2. Evaluation results for mAP@0.5 and mAP@0.5:0.95 for different hyperparameter conﬁgura- tions (HC), for default values (D) and optimised values with evolution (E). It seems that there was not much difference in all conﬁgurations. Based on mAP@50 and mAP@50:95, conﬁgurations with hyperparameters derived from evolution showed a slightly better performance. Moreover, conﬁgurations based on YOLOv5l returned a slightly better performance. The model based on YOLOv5l with hyperparameters de- rived from evolution and with a batch size of eight achieved the best performance. More speciﬁcally, it reached an mAP@0.5 of 0.79353 and an mAP@0.5:0.95 of 0.54582. Another type of evaluation was based on the F1-Score metric. Table 3 shows the comparison of all conﬁgurations based on the best occurrence of the F1-Score. Better performance on this metric was achieved by the same conﬁguration with YOLOv5l, hy- perparameters derived from evolution, and a batch size of eight, as it reached an F1-Score of 76.65%. Table 3. Evaluation results for the F1-Score for different hyperparameter conﬁgurations (HC), for default values (D) and optimised values with evolution (E). Figure 7 shows the curve of the F1-Score for the best conﬁguration. The horizontal axis corresponds to the conﬁdence of the detected mushrooms, while the vertical axis corresponds to the F1-score. In this example, the maximum F1-score had a conﬁdence of 0.566. In addition, the graph shows that the class “Stage1” had a better accuracy. This was expected since all photos with mushrooms belonging to “Stage1” were simpler, as mushrooms were small and not overlapping. To notice the detection accuracy for each class more clearly, Figure 8 presents the confusion matrix for the same conﬁguration. It shows that class “Stage1” had an accuracy of 82%, class “Stage2” had an accuracy of 71%, and “Stage3” had an accuracy of 70%. Although this does not seem perfect, we can say that it was good enough depending on the complexity of the photos with mushrooms in “Stage2” and “Stage3”. Figure 9 shows an example of the detection where only mushrooms from classes Stage2 and Stage3 exist. The green bounding box indicates that the mushroom is ready to harvest (Stage3). The blue bounding box indicates that the mushroom is not yet ready to harvest (Stage2). We marked in magenta the mushrooms that were detected in the wrong class. Moreover, we marked in red the mushrooms that were not detected. It seems from the photo that the mushrooms in the foreground were detected in the right class, while some errors occurred in mushrooms that were in the background or partially displayed. After the procedure described previously, we obtained 575 different masks from 114 different mushrooms in the greenhouse. All masks were distinguished for different mushrooms depending on the number of the substrate grow bag and the position on it. When the mask corresponded to the ﬁrst day of appearance of each mushroom, it was marked as day one. The corresponding masks of the next days of the same mushroom were numbered as well in the same way. Figure 10 shows the growth rate of four different mushrooms. Each mushroom is indicated with a different colour. The horizontal axis of the graph contains the number of the growth day, starting from number one for the ﬁrst day of the mushroom’s appearance. The vertical axis indicates the size of the mushroom counted in pixels. Moreover, we calculated the maximum size of each mushroom based on all photos before it was harvested. We set its size as 100%, and we calculated the size of the same mushroom for all other days accordingly. Figure 11 shows the growth rate of the same mushrooms but as a percentage of the maximum size of each mushroom. Both graphs show that the three mushrooms (M1, M2, M3) followed an almost linear growth rate. The fourth mushroom (M4) also followed a linear growth rate for the ﬁrst four days, but from the ﬁfth day, it seemed not to change in size. In fact, on the ﬁfth day, that speciﬁc mushroom should have been harvested, but it was accidentally forgotten. Figure 12 shows the captured photos of this mushroom (M4), from which it seemed that the mushroom was ready to harvest on day ﬁve. Figure 12. Mushroom growth example photos. We manually evaluated the harvesting status of each mushroom and decided that 17 mushrooms could be harvested earlier, which represented 14.04% of the mushrooms. The average difference between the size of the masks from the day identiﬁed as ready to harvest and the previous day was 5.34%. Thus, if we set it as a threshold in our method to provide an alert any time that the difference in the mushroom size was below it, we could inform the farmer of the harvest time. This would lead to an improvement in the quality of the harvested mushrooms as we would avoid overripe mushrooms. In addition, our evaluation showed that the average days for the growth of mushrooms was 5.22. Figure 13 displays the graph with all mushrooms detected for all days. All values are displayed in percentage of the maximum size of the speciﬁc mushroom. The blue marks indicate the detected mushrooms that were not ready to harvest, while the red marks indicate mushrooms that should have been harvested at least one day before. ===discussion=== Our ﬁrst approach for mushroom detection with YOLOv5 could be a valuable part of a decision support system. For example, it may be useful in tasks such as yield prediction or product improvement by indicating those mushrooms that are ready for harvest. Overall, the accuracy of the trained model was considered acceptable regarding the complexity of the photos of the dataset. However, our experience shows that some changes could improve the results. A more precisely selected dataset should improve the detection accuracy of different classes. For example, photos with fewer mushrooms and only in the foreground will give more accurate results. As we discussed in Figure 9, the main drawback in detection accuracy is that many mushrooms are partially visible in the photos, and many of them are in the background, which makes it difﬁcult to detect and classify because their features are not clear. Moreover, a proposed methodology to overcome these obstacles could be to exclude mushrooms that are partially displayed in the picture and to focus only on those clearly displayed. Furthermore, the 600 steps we used for the hyperparameter evolution in this research may not be good enough, and more steps will result in a better detection accuracy. This would lead to better combinations of the hyperparameters, but the hyperparameter evolu- tion procedure is a very time-consuming task, even for a professional graphic processing unit. A better approach to enhance the model’s accuracy is to modify the YOLOv5 algorithm. Thus, the trained model would include the speciﬁc characteristics of the different classes. Our second approach for mushroom growth monitoring could also be helpful for a decision support system to inform farmers about the harvesting time. As a result, an improvement in product quality is expected since mushrooms will be collected at the right time. Furthermore, the precisely detected mask of the mushrooms may support other potential applications. For example, it could be a useful part of a robotic mechanism in order to detect, decide, and collect only those mushrooms that are ready to harvest without damaging them. The method of mushroom growth monitoring with Detectron2 also has some limi- tations. Our method focused only on mushrooms that were on top of the substrate bag, which was placed horizontally in the greenhouse. In fact, only one-third of the mushrooms could be observed with this method since there were also substrate bags positioned verti- cally below the substrate bag we used. In addition, in many cases, the substrate bags are densely placed in the greenhouse, and sometimes, the farmers use more than one layer of substrate grow bags in the greenhouse, making it difﬁcult to capture photos even from those positioned horizontally. Furthermore, getting only one image per day has the disadvantage that the growing days may not be counted precisely in some mushrooms. For example, some mushrooms may appear a few hours after a picture was taken, resulting in a day count less than the actual growth duration. Moreover, some mushrooms may have been harvested just before a picture was taken, also resulting in reduced count days. Therefore, capturing photos of the substrate bags more often could give better results and inform more precisely about the growing rate of the mushroom and the harvesting time. Finally, the results of the mushroom detection are not always precise. In fact, the proposed method has two minor drawbacks. First, some of the mushrooms that were too small could not be detected. In addition, the masks returned from Detectron2 were not always accurate. We believe that both these drawbacks do not have a large impact on the proposed methodology. However, if we need to improve them, we could use a larger dataset with more annotated mushrooms to get better results. ===conclusions=== Object detection and instance segmentation have multiple applications in various domains. In smart farming, it is mainly used in fruit detection, weed detection, or pest detection. In this paper, we evaluated the effectiveness of YOLOv5 and Detectron2 in mushroom detection in a greenhouse. Firstly, we evaluated YOLOv5 on its effectiveness in classifying mushrooms in three growth stages. The results showed that even in complex environments such as a greenhouse with Pleurotus mushrooms, it was possible to identify mushrooms that were ready to harvest. The evaluation results on mushroom detection and classiﬁcation in three different growing stages gave an F1-score of up to 76.5%, and speciﬁcally for the ﬁnal growing stage, an accuracy of up to 70%. Secondly, we proposed a method for mushroom growth monitoring. For that purpose, we used two trained models with Detectron2. The ﬁrst one detected the substrate grow bag, and the second detected the mushrooms and returned the corresponding masks. Experi- mental results showed that the growth rate of Pleurotus mushrooms was linear. Moreover, this procedure made it possible to detect when mushrooms reached their maximum size and were ready to harvest. The evaluation results showed that it was possible to make de- cisions and improve harvesting time for up to 14.04% of the mushrooms in the greenhouse. In addition, the results showed that, on average, Pleurotus mushrooms needed 5.22 days to reach maximum size. Moreover, the proposed methods are suitable to be part of a decision support system to inform farmers about the status of their cultivation. In addition, a future potential application would be a robotic mechanism able to detect and recognise the growth stage of mushrooms before harvesting them. Author Contributions: Conceptualization, V.M. and P.S.; methodology, V.M. and G.K.; software, V.M.; validation, G.K. and P.S.; formal analysis, V.M.; investigation, V.M.; resources, V.M. and N.M.; data curation, V.M. and G.K.; writing—original draft preparation, V.M.; writing—review and editing, S.B. and I.M.; visualization, V.M.; supervision, P.S.; project administration, P.S.; funding acquisition, P.S. All authors have read and agreed to the published version of the manuscript. Funding: This research was cofunded by the European Regional Development Fund of the European Union and Greek national funds through the Operational Program Western Macedonia 2014–2020, under the call “Collaborative and networking actions between research institutions, educational institutions and companies in priority areas of the strategic smart specialization plan of the region”, project “Smart Mushroom fARming with internet of Things—SMART”, project code: DMR-0016521. Data Availability Statement: The data presented in this study are available on request from the corresponding author. The data are not publicly available due to restrictions on privacy. ===references=== Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e68910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract: Mushrooms contain valuable nutrients, proteins, minerals, and vitamins, and it is suggested to include them in our diet. Many farmers grow mushrooms in restricted environments with speciﬁc atmospheric parameters in greenhouses. In addition, recent technologies of the Internet of things intend to give solutions in the agriculture area. In this paper, we evaluate the effectiveness of machine learning for mushroom growth monitoring for the genus Pleurotus. We use YOLOv5 to detect mushrooms’ growing stage and indicate those ready to harvest. The results show that it can detect mushrooms in the greenhouse with an F1-score of up to 76.5%. The classiﬁcation in the ﬁnal stage of mushroom growth gives an accuracy of up to 70%, which is acceptable considering the complexity of the photos used. In addition, we propose a method for mushroom growth monitoring based on Detectron2. Our method shows that the average growth period of the mushrooms is 5.22 days. Moreover, our method is also adequate to indicate the harvesting day. The evaluation results show that it could improve the time to harvest for 14.04% of the mushrooms. ',\n",
       " 'introduction',\n",
       " ' Citation: Moysiadis, V.; Kokkonis, G.; Bibi, S.; Moscholios, I.; Maropoulos, N.; Sarigiannidis, P. Monitoring Mushroom Growth with Machine Learning. Agriculture 2023, 13, 223. https://doi.org/10.3390/ agriculture13010223 Copyright: © 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). Mushrooms are a rich source of nutrients, proteins, minerals, and vitamins (B, C, and D) [1]. More than 200 species of edible mushrooms are used as ingredients in traditional foods around the world [2], but only 35 species are cultivated in greenhouses in restricted environments [3]. The mushrooms of the genus Pleurotus are in second place worldwide in the industry. Therefore, improving the production line will have a major impact on the economy of the speciﬁc domain. The authors in [4] presented a comprehensive review of the factors affecting the mushrooms of the genus Pleurotus spp. In addition, many research efforts have focused on solutions based on modern technologies from the Internet of things, aiming to transform traditional farming into the new era called smart farming [5]. This transition brings many applications in various agriculture tasks aiming to increase production, reduce cost production, reduce chemical inputs, and reduce labour effort. Along with other technologies, machine learning is highly used in tasks such as yield prediction, disease detection, weed recognition, and fruit recognition. More speciﬁcally, many research efforts try to classify and detect the speciﬁc location of different objects in images. For example, a robotic mechanism is suitable for weeding if it has the ability to recognise weeds from vegetation [6]. The detection of fruits with machine learning is also valuable in robotic mechanisms [7–9]. Disease detection has already seen numerous research efforts evaluating the ability to detect various diseases in various crops [10–12]. Moreover, insect detection is another important task in cultivation. Thus, many research efforts exist in this area [13,14]. Furthermore, machine learning is already used in research efforts in mushroom culti- vation or wild mushroom hunting, aiming to give solutions to various tasks. The authors in [15] gave a comprehensive review in this area. One of the main tasks based on machine learning focuses on species recognition. For example, the authors in [16] provided a method to distinguish edible mushrooms from poisonous ones. In addition, the authors in [17] suggested a method for species recognition with a mobile smart device. Another research effort on mushroom species classiﬁcation is presented in [18]. Furthermore, a few research efforts have used object detection algorithms in order to localise and classify mushrooms in a photo. The authors in [19] developed a system that used object detection to localise and recognise mushrooms that were ready to harvest. In [20], the authors evaluated the poten- tial of mushroom hunting using a custom-made unmanned aerial vehicle (UAV). Finally, the authors in [21] presented an automatic robotic mechanism for mushroom harvesting using machine learning for detection. Object detection algorithms have gained signiﬁcant momentum in recent years due to the success of deep learning [22], a speciﬁc area of machine learning. As a result, many object detection algorithms have been available in recent years that can detect and provide the exact location of an object in an image or video. In addition, some of them are also capable of providing a corresponding mask of the object. This category is called instance segmentation. Detectron2 [23] is the most popular in this category, while some other variants, such as Mask Scoring RCNN [24], try to return better results of the provided masks. In addition, YOLOv5 [25] is one of the most popular object detection algorithms, delivering accurate and rapid results. With recent technologies, a sufﬁcient number of research efforts are available for mush- room cultivation. Most of them propose controlling growing conditions in the greenhouse and keeping them within speciﬁc boundaries [26,27]. However, only a few of the research efforts go a step over and use more advanced mushroom cultivation methods. Machine learning promise to provide efﬁcient methods in this direction. Thus, we should use it to help farmers in their everyday activities and support their decisions. Until now, only a few research efforts have used machine learning in the mushroom industry. As described before, most of them deal with mushroom species classiﬁcation, and only a few try to give solutions for usable tasks such as mushroom harvesting in the greenhouse. Moreover, none of the existing methods for mushroom harvesting utilises the genus Pleurotus. Our work uses machine learning not only to detect the mushrooms in the greenhouse but also to classify them in three different stages depending on their growth status and predict when they are ready to harvest. For this purpose, we use the YOLOv5 [25] object detection algorithm and evaluate its accuracy on mushroom detection with different con- ﬁgurations of hyperparameters. The evaluation results show that YOLOv5 can detect and classify mushrooms with an F1-score of up to 76.5% and detect the ﬁnal stage of mushroom growth with an accuracy of up to 70%. Furthermore, in the second part of our research, we use Detectron2 [23] to extract a corresponding mask for each mushroom and use it to calculate its size and monitor the growth rate. The results show that they follow a linear growth rate, and it is also possible to predict the harvesting day based on images on previous days. In addition, the method can improve the harvesting time for 14.04% of the mushrooms. Our work could be a valuable method for mushroom detection on greenhouses with practical applications. For example, it could be used for yield prediction, where ground cameras can observe the greenhouse and predict the yield a few days before the mushrooms are ready to harvest. Another possible application could be on robotics mechanisms that can detect the exact crown of the mushrooms in order to harvest them with no damage. This work is part of our decision support system described in our previous work [28], where we presented a system architecture that covered mushroom cultivation in a greenhouse and the aggregation of useful information for wild mushroom hunting. The rest of this paper is as follows. In Section 2, we give the most relevant research efforts for mushroom detection using machine learning or image processing. Section 3 brieﬂy discusses the basic concepts of object detection and instance segmentation and provides the essential features of YOLOv5 and Detectron2. Section 4 analyses the method- ology we follow in this research. Next, Section 5 presents the evaluation results between different conﬁgurations of YOLOv5 and its effectiveness on mushroom classiﬁcation at different stages of mushroom growth. In addition, we provide the results from Detectron2 on mushroom growth monitoring. Section 6 discusses the effectiveness of the proposed methods, their limitations and possible impact on future applications. Finally, Section 7 concludes this paper. ',\n",
       " 'related work',\n",
       " ' In this section, we brieﬂy present similar research efforts in the area of smart farming, particularly in image processing or machine learning for image segmentation or object recognition on mushroom cultivation. A harvesting robot for oyster mushrooms was proposed in [21]. The development system used an improved SSD algorithm to detect mushrooms ready to harvest. The algorithm used RGB images and point clouds collected by an Intel RealSense D435i camera. The evaluation results showed an accuracy of mushroom recognition of up to 95.0% and a harvesting success rate of up to 86.8%. The average harvesting time for a single mushroom was 8.85 s. The authors in [19] provided a measurement monitoring system to observe mushroom growth in a greenhouse. The proposed system used YOLOv3 for mushroom detection and an additional localisation method to improve the position of the detected mushroom in order to distinguish the same mushroom in different captured photos. Moreover, the system was able to estimate the harvesting time. In [16], the authors presented a mushroom farm automation to classify toxic mush- rooms. The proposed system adopted machine learning to distinguish edible mushrooms from poisonous mushrooms. The utilised model was a combination of six different clas- siﬁers that worked together and reach an accuracy of 100%. More speciﬁcally, the en- semble model used the following classiﬁers: decision tree (DT), logistic regression (LR), K-nearest neighbours (KNN), support vector machine (SVM), naive Bayes (NB), and ran- dom forest (RF). Moreover, the manuscript introduced an architectural design for smart mushroom farming. The authors in [29] presented an automatic sorting system for fresh, white button mushrooms. Apart from the automatic mechanism, they proposed an image processing algorithm to collect button mushrooms. The algorithm eliminated the shadow and petiole on the image and determined the pileus diameter of the mushrooms. Experimental results showed that compared to manual grading speed, their approach was improved by 38.86%, and the accuracy was improved by 6.84%. ',\n",
       " 'background',\n",
       " ' In this section, we brieﬂy presenting the main characteristics of YOLOv5 [25] and Detectron2 [23]. Both of them are suitable for object detection in images or videos. Moreover, they both belong to object detection algorithms based on convolutional neural networks. Thus, they return the class and the position of the detected object. Furthermore, Detectron2 is also an instance segmentation algorithm able to return a corresponding mask that deﬁnes the area of the object. YOLOv5 [25] comes from You Only Look Once and claims to be one of the fastest object detection algorithms. In fact, it is one of the best-known object detection algorithms due to its speed and accuracy. It is divided into three components, namely, a backbone, neck and head, as all single-stage detectors. In particular, the ﬁrst component contains a backbone network that extracts rich feature representations from images. The second component consists of the model neck responsible for extracting feature pyramids that help to recognise objects of different sizes and scales. The ﬁnal component is the model head that applies anchor boxes on feature maps. In addition, it is responsible for rendering the predicted class, the scores of the predicted objects, and the bounding boxes. Detectron2 [23] is considered state-of-the-art in instance segmentation. It is the succes- sor of Mask RCNN [30], which is built on top of Faster RCNN. Thus, apart from providing The architecture of Mask RCNN is divided into three stages. In the ﬁrst stage, a regional proposal network (RPN) is responsible for returning all regions of possible areas with detected objects. Mask RCNN uses ResNet50 or ResNet101 with FPN support as a backbone network in this stage. In the second stage, a classiﬁer is responsible for evaluating the region proposals derived from the ﬁrst stage and providing the predictions with bounding boxes for each detected object. Finally, the third stage provides the corresponding masks of the detected objects. ',\n",
       " 'materials and methods',\n",
       " ' This section presents the methodology we followed in our research. First, we describe the image acquisition from mushrooms in the greenhouse and the annotation process. Second, we give the basic conﬁguration of YOLOv5 for mushroom classiﬁcation in three different growing stages. Moreover, we provide the architecture we used for mushroom growth monitoring. Data collection took place in a greenhouse near the city of Grevena, Greece. We collected multiple images in a greenhouse, each containing one or multiple mushrooms. We captured only images that had mushrooms mainly in the foreground and not in the background because they reduced the detection accuracy. Finally, we obtained 1128 images with one or more mushrooms. We annotated them into three different classes (Stage1, Stage2, Stage3) depending on the growing stage. In more detail, the ﬁrst stage corresponded to the ﬁrst days of appearance, when mushrooms were too small and they had just started to form their shape. The second class corresponded to mushrooms that had already formed their shape and continued to grow. Some of them were obviously small, while others were big enough but not ready to harvest. The third class contained mushrooms that were ready to harvest, which could be manually indicated from two parameters. More speciﬁcally, when the edges of their caps started to become ﬂat or slightly uprolled, it was an indication of their ﬁnal stage of growth [31]. Figure 1 shows some examples of mushrooms belonging to the three different stages. Many of the collected photos in the greenhouse contained multiple mushrooms, making the annotation complex and the recognition even more challenging, especially when the mushrooms were at the ﬁnal stage and many of them were overlapping. Figure 2 presents two of those photos as an example. Figure 2. Pictures from the greenhouse with multiple mushrooms. Finally, 4271 mushrooms were annotated and classiﬁed into three different classes (Stage1, Stage2, Stage3) as described before. Moreover, 1130 of them belonged to the ﬁrst growing stage, 1845 to the second growing stage, and 1296 of them to the third growing stage. In addition, the 1128 annotated photos were divided into 784 for the training dataset and 344 for the validation dataset. For the second part of our research, we collected a different set of photos. In particular, the data acquisition for the growing rate of mushrooms was made in 33 different mushroom substrate grow bags. We captured a photo for each one on seven different days between 16 August 2022 and 24 August 2022. Finally, we collected 231 different photos in total. The distance between the camera and the substrate grow bag was one meter above. Figure 3 shows an example of three captured photos. We did not annotate these photos, but we used the trained models from Detectron2 to indicate the size of each mushroom. In our evaluation for mushroom growth monitoring, we used only the mushrooms that were on top of the substrate grow bag. For training Detectron2 for instance segmentation, we annotated another two datasets. The ﬁrst one contained annotations for substrate grow bags, and the second one contained annotations for mushrooms. The annotation process for the image segmentation is a time- consuming task since we had to annotate precisely the objects with polygons. For that reason, we chose to annotate only photos that contained a small number of mushrooms to simplify the annotation. Finally, we annotated 453 mushrooms and divided them into 358 for the training dataset and 95 for the validation dataset. In addition, for the detection of substrate grow bags, we annotated 200 photos and divided them into 150 for the training dataset and 50 for the validation dataset. For mushroom detection and classiﬁcation in the three different classes as described previously, we used YOLOv5. First, we trained our models with the default hyperparame- ters using the pretrained models YOLOv5s and YOLOv5l with an image size of 640 pixels and batch sizes of 2, 4, and 8. To achieve better results, we used the provided method integrated with YOLOv5 for hyperparameter optimisation called hyperparameter evolution. It is a genetic algorithm to ﬁnd the best set of hyperparameters for the speciﬁc dataset. The evolving procedure starts from the default hyperparameters or other user-deﬁned values, if available, and tries to improve a ﬁtness function in every loop. The default ﬁtness function is a weighted combination of mAP@0.5 with 10% contribution and mAP@0.5:0.95 with 90% contribution. In every evolving loop, the primary genetic operator is the mutation. The proposed combination for the mutation uses 80% probability and a 0.04 variance to calculate the next combination of hyperparameters based on the best parents from previous generations. We used the hyperparameter evolution approach for the two pretrained models, YOLOv5s and YOLOv5l, and trained them for 600 generations with an image size of 640 pixels and a batch size of 4. Table 1 shows the predicted sets of hyperparameters for each pretrained model. Table 1. Optimised hyperparameters after evolving for YOLOv5s and YOLOv5l. Figure 4 shows a graphical representation for YOLOv5l with each hyperparameter displayed in a different subplot. Each subplot presents all values from all generations for the speciﬁc hyperparameter. The horizontal axis corresponds to the value of the hyperparameter, and the vertical value corresponds to the calculated ﬁtness. The yellow areas indicate a high concentration of values. Subplots with vertical distributions indicate that the speciﬁc hyperparameter was disabled and did not mutate. Figure 4. Graphical representation for the calculated values of all hyperparameters, after evolving for 600 generations for YOLOv5l. The mAP@0.5 metric corresponds to the average precision over a threshold of 0.5 for the object detection. In addition, the mAP@0.5:0.95 denotes the average precision with a threshold between 0.5 and 0.95. The precision metric indicates the correctly identiﬁed trees divided by the total number of detected trees and is given by Equation (1). The recall metric indicates the falsely identi- ﬁed trees divided by the total number of actual existing trees and is given by Equation (2). where TP (true positive) is the number of correctly detected objects in the class, FP (false positive) is the number of falsely detected objects in the speciﬁc class, and FN (false negative) is the number of objects that are not detected in the speciﬁc class. For mushroom growth monitoring, we used two different stages for the object detec- tion. The ﬁrst one was responsible for detecting the substrate grow bag, and the second one was responsible for detecting the mushrooms in a photo. Figure 5 illustrates the main components of the architecture of the proposed method. More speciﬁcally, in the stage of the substrate grow bag detection, we use the trained model with Detectron2. We only obtained the bounding box of the detected substrate grow bag and not the mask. After the detection of the substrate grow bag, we used the corresponding bounding box and divided it into six equal rectangles (2 × 3). Figure 6a illustrates the bounding box (green) and the lines (red) that divide the bounding box in order to distinguish the detected mushrooms. Moreover, we used the trained model with Detectron2 to detect the mushrooms in the photo and return the corresponding masks. Figure 6b displays the detected mush- rooms. Next, we performed a resize operation on the bounding box and the masks to achieve normalisation for calculating the size in all photos of the same substrate grow bag. Furthermore, the detected mushrooms were distinguished in one of the rectangles with the following rules. A mushroom should have more than 50% in the speciﬁc rectangle. Only one mushroom can belong in one rectangle, and each mushroom belongs only in one rectangle. In any case, the selected mushroom was the one that covered the most area in the rectangle. Finally, Figure 6c illustrates the divided bounding box with the corresponding masks of all detected masks. The size of the provided mask could indicate the size of the mushroom. After distinguishing each mushroom, we calculated the coverage based on the pixels of the provided mask and used it for mushroom growth monitoring. ',\n",
       " 'results',\n",
       " ' In the following subsections, we evaluate the results of the mushroom classiﬁcation for different growth stages. Next, we provide the results of mushroom monitoring for their growth rate. In this subsection, we compared different conﬁgurations of YOLOv5 for mushroom detection and classiﬁcation in three classes, as described previously. We used the metrics mAP@0.5, mAP@0.5:0.95, precision, recall, and F1-score to compare the accuracy of each trained model. Table 2 shows a comparison of all conﬁgurations. More speciﬁcally, we use the pretrained models YOLOv5s and YOLOv5l with the default (D) conﬁguration for the hyperparameters and with the conﬁguration derived from evolution (E). Moreover, our conﬁgurations used an image size of 640 pixels and three different values for the batch size (2, 4, 8). Table 2. Evaluation results for mAP@0.5 and mAP@0.5:0.95 for different hyperparameter conﬁgura- tions (HC), for default values (D) and optimised values with evolution (E). It seems that there was not much difference in all conﬁgurations. Based on mAP@50 and mAP@50:95, conﬁgurations with hyperparameters derived from evolution showed a slightly better performance. Moreover, conﬁgurations based on YOLOv5l returned a slightly better performance. The model based on YOLOv5l with hyperparameters de- rived from evolution and with a batch size of eight achieved the best performance. More speciﬁcally, it reached an mAP@0.5 of 0.79353 and an mAP@0.5:0.95 of 0.54582. Another type of evaluation was based on the F1-Score metric. Table 3 shows the comparison of all conﬁgurations based on the best occurrence of the F1-Score. Better performance on this metric was achieved by the same conﬁguration with YOLOv5l, hy- perparameters derived from evolution, and a batch size of eight, as it reached an F1-Score of 76.65%. Table 3. Evaluation results for the F1-Score for different hyperparameter conﬁgurations (HC), for default values (D) and optimised values with evolution (E). Figure 7 shows the curve of the F1-Score for the best conﬁguration. The horizontal axis corresponds to the conﬁdence of the detected mushrooms, while the vertical axis corresponds to the F1-score. In this example, the maximum F1-score had a conﬁdence of 0.566. In addition, the graph shows that the class “Stage1” had a better accuracy. This was expected since all photos with mushrooms belonging to “Stage1” were simpler, as mushrooms were small and not overlapping. To notice the detection accuracy for each class more clearly, Figure 8 presents the confusion matrix for the same conﬁguration. It shows that class “Stage1” had an accuracy of 82%, class “Stage2” had an accuracy of 71%, and “Stage3” had an accuracy of 70%. Although this does not seem perfect, we can say that it was good enough depending on the complexity of the photos with mushrooms in “Stage2” and “Stage3”. Figure 9 shows an example of the detection where only mushrooms from classes Stage2 and Stage3 exist. The green bounding box indicates that the mushroom is ready to harvest (Stage3). The blue bounding box indicates that the mushroom is not yet ready to harvest (Stage2). We marked in magenta the mushrooms that were detected in the wrong class. Moreover, we marked in red the mushrooms that were not detected. It seems from the photo that the mushrooms in the foreground were detected in the right class, while some errors occurred in mushrooms that were in the background or partially displayed. After the procedure described previously, we obtained 575 different masks from 114 different mushrooms in the greenhouse. All masks were distinguished for different mushrooms depending on the number of the substrate grow bag and the position on it. When the mask corresponded to the ﬁrst day of appearance of each mushroom, it was marked as day one. The corresponding masks of the next days of the same mushroom were numbered as well in the same way. Figure 10 shows the growth rate of four different mushrooms. Each mushroom is indicated with a different colour. The horizontal axis of the graph contains the number of the growth day, starting from number one for the ﬁrst day of the mushroom’s appearance. The vertical axis indicates the size of the mushroom counted in pixels. Moreover, we calculated the maximum size of each mushroom based on all photos before it was harvested. We set its size as 100%, and we calculated the size of the same mushroom for all other days accordingly. Figure 11 shows the growth rate of the same mushrooms but as a percentage of the maximum size of each mushroom. Both graphs show that the three mushrooms (M1, M2, M3) followed an almost linear growth rate. The fourth mushroom (M4) also followed a linear growth rate for the ﬁrst four days, but from the ﬁfth day, it seemed not to change in size. In fact, on the ﬁfth day, that speciﬁc mushroom should have been harvested, but it was accidentally forgotten. Figure 12 shows the captured photos of this mushroom (M4), from which it seemed that the mushroom was ready to harvest on day ﬁve. Figure 12. Mushroom growth example photos. We manually evaluated the harvesting status of each mushroom and decided that 17 mushrooms could be harvested earlier, which represented 14.04% of the mushrooms. The average difference between the size of the masks from the day identiﬁed as ready to harvest and the previous day was 5.34%. Thus, if we set it as a threshold in our method to provide an alert any time that the difference in the mushroom size was below it, we could inform the farmer of the harvest time. This would lead to an improvement in the quality of the harvested mushrooms as we would avoid overripe mushrooms. In addition, our evaluation showed that the average days for the growth of mushrooms was 5.22. Figure 13 displays the graph with all mushrooms detected for all days. All values are displayed in percentage of the maximum size of the speciﬁc mushroom. The blue marks indicate the detected mushrooms that were not ready to harvest, while the red marks indicate mushrooms that should have been harvested at least one day before. ',\n",
       " 'discussion',\n",
       " ' Our ﬁrst approach for mushroom detection with YOLOv5 could be a valuable part of a decision support system. For example, it may be useful in tasks such as yield prediction or product improvement by indicating those mushrooms that are ready for harvest. Overall, the accuracy of the trained model was considered acceptable regarding the complexity of the photos of the dataset. However, our experience shows that some changes could improve the results. A more precisely selected dataset should improve the detection accuracy of different classes. For example, photos with fewer mushrooms and only in the foreground will give more accurate results. As we discussed in Figure 9, the main drawback in detection accuracy is that many mushrooms are partially visible in the photos, and many of them are in the background, which makes it difﬁcult to detect and classify because their features are not clear. Moreover, a proposed methodology to overcome these obstacles could be to exclude mushrooms that are partially displayed in the picture and to focus only on those clearly displayed. Furthermore, the 600 steps we used for the hyperparameter evolution in this research may not be good enough, and more steps will result in a better detection accuracy. This would lead to better combinations of the hyperparameters, but the hyperparameter evolu- tion procedure is a very time-consuming task, even for a professional graphic processing unit. A better approach to enhance the model’s accuracy is to modify the YOLOv5 algorithm. Thus, the trained model would include the speciﬁc characteristics of the different classes. Our second approach for mushroom growth monitoring could also be helpful for a decision support system to inform farmers about the harvesting time. As a result, an improvement in product quality is expected since mushrooms will be collected at the right time. Furthermore, the precisely detected mask of the mushrooms may support other potential applications. For example, it could be a useful part of a robotic mechanism in order to detect, decide, and collect only those mushrooms that are ready to harvest without damaging them. The method of mushroom growth monitoring with Detectron2 also has some limi- tations. Our method focused only on mushrooms that were on top of the substrate bag, which was placed horizontally in the greenhouse. In fact, only one-third of the mushrooms could be observed with this method since there were also substrate bags positioned verti- cally below the substrate bag we used. In addition, in many cases, the substrate bags are densely placed in the greenhouse, and sometimes, the farmers use more than one layer of substrate grow bags in the greenhouse, making it difﬁcult to capture photos even from those positioned horizontally. Furthermore, getting only one image per day has the disadvantage that the growing days may not be counted precisely in some mushrooms. For example, some mushrooms may appear a few hours after a picture was taken, resulting in a day count less than the actual growth duration. Moreover, some mushrooms may have been harvested just before a picture was taken, also resulting in reduced count days. Therefore, capturing photos of the substrate bags more often could give better results and inform more precisely about the growing rate of the mushroom and the harvesting time. Finally, the results of the mushroom detection are not always precise. In fact, the proposed method has two minor drawbacks. First, some of the mushrooms that were too small could not be detected. In addition, the masks returned from Detectron2 were not always accurate. We believe that both these drawbacks do not have a large impact on the proposed methodology. However, if we need to improve them, we could use a larger dataset with more annotated mushrooms to get better results. ',\n",
       " 'conclusions',\n",
       " ' Object detection and instance segmentation have multiple applications in various domains. In smart farming, it is mainly used in fruit detection, weed detection, or pest detection. In this paper, we evaluated the effectiveness of YOLOv5 and Detectron2 in mushroom detection in a greenhouse. Firstly, we evaluated YOLOv5 on its effectiveness in classifying mushrooms in three growth stages. The results showed that even in complex environments such as a greenhouse with Pleurotus mushrooms, it was possible to identify mushrooms that were ready to harvest. The evaluation results on mushroom detection and classiﬁcation in three different growing stages gave an F1-score of up to 76.5%, and speciﬁcally for the ﬁnal growing stage, an accuracy of up to 70%. Secondly, we proposed a method for mushroom growth monitoring. For that purpose, we used two trained models with Detectron2. The ﬁrst one detected the substrate grow bag, and the second detected the mushrooms and returned the corresponding masks. Experi- mental results showed that the growth rate of Pleurotus mushrooms was linear. Moreover, this procedure made it possible to detect when mushrooms reached their maximum size and were ready to harvest. The evaluation results showed that it was possible to make de- cisions and improve harvesting time for up to 14.04% of the mushrooms in the greenhouse. In addition, the results showed that, on average, Pleurotus mushrooms needed 5.22 days to reach maximum size. Moreover, the proposed methods are suitable to be part of a decision support system to inform farmers about the status of their cultivation. In addition, a future potential application would be a robotic mechanism able to detect and recognise the growth stage of mushrooms before harvesting them. Author Contributions: Conceptualization, V.M. and P.S.; methodology, V.M. and G.K.; software, V.M.; validation, G.K. and P.S.; formal analysis, V.M.; investigation, V.M.; resources, V.M. and N.M.; data curation, V.M. and G.K.; writing—original draft preparation, V.M.; writing—review and editing, S.B. and I.M.; visualization, V.M.; supervision, P.S.; project administration, P.S.; funding acquisition, P.S. All authors have read and agreed to the published version of the manuscript. Funding: This research was cofunded by the European Regional Development Fund of the European Union and Greek national funds through the Operational Program Western Macedonia 2014–2020, under the call “Collaborative and networking actions between research institutions, educational institutions and companies in priority areas of the strategic smart specialization plan of the region”, project “Smart Mushroom fARming with internet of Things—SMART”, project code: DMR-0016521. Data Availability Statement: The data presented in this study are available on request from the corresponding author. The data are not publicly available due to restrictions on privacy. ',\n",
       " 'references',\n",
       " ' Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = d[\"text\"]\n",
    "parts = text.split(\"===\")\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22ef4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of \"introduction\" and \"references\"\n",
    "introduction_index = next(i for i, section in enumerate(parts) if section == 'introduction')\n",
    "references_index = next(i for i, section in enumerate(parts) if section == 'references')\n",
    "\n",
    "# Remove sections before \"introduction\" and after \"references\"\n",
    "parts_with_chapters = parts[introduction_index:references_index]\n",
    "\n",
    "# Combine the remaining sections with text above 100 characters\n",
    "parts_without_chapters = [part for part in parts_with_chapters if len(part) > 100]\n",
    "\n",
    "combined_parts_without_chapters = \"\\n\".join(parts_without_chapters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0a8ada1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction',\n",
       " ' Citation: Moysiadis, V.; Kokkonis, G.; Bibi, S.; Moscholios, I.; Maropoulos, N.; Sarigiannidis, P. Monitoring Mushroom Growth with Machine Learning. Agriculture 2023, 13, 223. https://doi.org/10.3390/ agriculture13010223 Copyright: © 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). Mushrooms are a rich source of nutrients, proteins, minerals, and vitamins (B, C, and D) [1]. More than 200 species of edible mushrooms are used as ingredients in traditional foods around the world [2], but only 35 species are cultivated in greenhouses in restricted environments [3]. The mushrooms of the genus Pleurotus are in second place worldwide in the industry. Therefore, improving the production line will have a major impact on the economy of the speciﬁc domain. The authors in [4] presented a comprehensive review of the factors affecting the mushrooms of the genus Pleurotus spp. In addition, many research efforts have focused on solutions based on modern technologies from the Internet of things, aiming to transform traditional farming into the new era called smart farming [5]. This transition brings many applications in various agriculture tasks aiming to increase production, reduce cost production, reduce chemical inputs, and reduce labour effort. Along with other technologies, machine learning is highly used in tasks such as yield prediction, disease detection, weed recognition, and fruit recognition. More speciﬁcally, many research efforts try to classify and detect the speciﬁc location of different objects in images. For example, a robotic mechanism is suitable for weeding if it has the ability to recognise weeds from vegetation [6]. The detection of fruits with machine learning is also valuable in robotic mechanisms [7–9]. Disease detection has already seen numerous research efforts evaluating the ability to detect various diseases in various crops [10–12]. Moreover, insect detection is another important task in cultivation. Thus, many research efforts exist in this area [13,14]. Furthermore, machine learning is already used in research efforts in mushroom culti- vation or wild mushroom hunting, aiming to give solutions to various tasks. The authors in [15] gave a comprehensive review in this area. One of the main tasks based on machine learning focuses on species recognition. For example, the authors in [16] provided a method to distinguish edible mushrooms from poisonous ones. In addition, the authors in [17] suggested a method for species recognition with a mobile smart device. Another research effort on mushroom species classiﬁcation is presented in [18]. Furthermore, a few research efforts have used object detection algorithms in order to localise and classify mushrooms in a photo. The authors in [19] developed a system that used object detection to localise and recognise mushrooms that were ready to harvest. In [20], the authors evaluated the poten- tial of mushroom hunting using a custom-made unmanned aerial vehicle (UAV). Finally, the authors in [21] presented an automatic robotic mechanism for mushroom harvesting using machine learning for detection. Object detection algorithms have gained signiﬁcant momentum in recent years due to the success of deep learning [22], a speciﬁc area of machine learning. As a result, many object detection algorithms have been available in recent years that can detect and provide the exact location of an object in an image or video. In addition, some of them are also capable of providing a corresponding mask of the object. This category is called instance segmentation. Detectron2 [23] is the most popular in this category, while some other variants, such as Mask Scoring RCNN [24], try to return better results of the provided masks. In addition, YOLOv5 [25] is one of the most popular object detection algorithms, delivering accurate and rapid results. With recent technologies, a sufﬁcient number of research efforts are available for mush- room cultivation. Most of them propose controlling growing conditions in the greenhouse and keeping them within speciﬁc boundaries [26,27]. However, only a few of the research efforts go a step over and use more advanced mushroom cultivation methods. Machine learning promise to provide efﬁcient methods in this direction. Thus, we should use it to help farmers in their everyday activities and support their decisions. Until now, only a few research efforts have used machine learning in the mushroom industry. As described before, most of them deal with mushroom species classiﬁcation, and only a few try to give solutions for usable tasks such as mushroom harvesting in the greenhouse. Moreover, none of the existing methods for mushroom harvesting utilises the genus Pleurotus. Our work uses machine learning not only to detect the mushrooms in the greenhouse but also to classify them in three different stages depending on their growth status and predict when they are ready to harvest. For this purpose, we use the YOLOv5 [25] object detection algorithm and evaluate its accuracy on mushroom detection with different con- ﬁgurations of hyperparameters. The evaluation results show that YOLOv5 can detect and classify mushrooms with an F1-score of up to 76.5% and detect the ﬁnal stage of mushroom growth with an accuracy of up to 70%. Furthermore, in the second part of our research, we use Detectron2 [23] to extract a corresponding mask for each mushroom and use it to calculate its size and monitor the growth rate. The results show that they follow a linear growth rate, and it is also possible to predict the harvesting day based on images on previous days. In addition, the method can improve the harvesting time for 14.04% of the mushrooms. Our work could be a valuable method for mushroom detection on greenhouses with practical applications. For example, it could be used for yield prediction, where ground cameras can observe the greenhouse and predict the yield a few days before the mushrooms are ready to harvest. Another possible application could be on robotics mechanisms that can detect the exact crown of the mushrooms in order to harvest them with no damage. This work is part of our decision support system described in our previous work [28], where we presented a system architecture that covered mushroom cultivation in a greenhouse and the aggregation of useful information for wild mushroom hunting. The rest of this paper is as follows. In Section 2, we give the most relevant research efforts for mushroom detection using machine learning or image processing. Section 3 brieﬂy discusses the basic concepts of object detection and instance segmentation and provides the essential features of YOLOv5 and Detectron2. Section 4 analyses the method- ology we follow in this research. Next, Section 5 presents the evaluation results between different conﬁgurations of YOLOv5 and its effectiveness on mushroom classiﬁcation at different stages of mushroom growth. In addition, we provide the results from Detectron2 on mushroom growth monitoring. Section 6 discusses the effectiveness of the proposed methods, their limitations and possible impact on future applications. Finally, Section 7 concludes this paper. ',\n",
       " 'related work',\n",
       " ' In this section, we brieﬂy present similar research efforts in the area of smart farming, particularly in image processing or machine learning for image segmentation or object recognition on mushroom cultivation. A harvesting robot for oyster mushrooms was proposed in [21]. The development system used an improved SSD algorithm to detect mushrooms ready to harvest. The algorithm used RGB images and point clouds collected by an Intel RealSense D435i camera. The evaluation results showed an accuracy of mushroom recognition of up to 95.0% and a harvesting success rate of up to 86.8%. The average harvesting time for a single mushroom was 8.85 s. The authors in [19] provided a measurement monitoring system to observe mushroom growth in a greenhouse. The proposed system used YOLOv3 for mushroom detection and an additional localisation method to improve the position of the detected mushroom in order to distinguish the same mushroom in different captured photos. Moreover, the system was able to estimate the harvesting time. In [16], the authors presented a mushroom farm automation to classify toxic mush- rooms. The proposed system adopted machine learning to distinguish edible mushrooms from poisonous mushrooms. The utilised model was a combination of six different clas- siﬁers that worked together and reach an accuracy of 100%. More speciﬁcally, the en- semble model used the following classiﬁers: decision tree (DT), logistic regression (LR), K-nearest neighbours (KNN), support vector machine (SVM), naive Bayes (NB), and ran- dom forest (RF). Moreover, the manuscript introduced an architectural design for smart mushroom farming. The authors in [29] presented an automatic sorting system for fresh, white button mushrooms. Apart from the automatic mechanism, they proposed an image processing algorithm to collect button mushrooms. The algorithm eliminated the shadow and petiole on the image and determined the pileus diameter of the mushrooms. Experimental results showed that compared to manual grading speed, their approach was improved by 38.86%, and the accuracy was improved by 6.84%. ',\n",
       " 'background',\n",
       " ' In this section, we brieﬂy presenting the main characteristics of YOLOv5 [25] and Detectron2 [23]. Both of them are suitable for object detection in images or videos. Moreover, they both belong to object detection algorithms based on convolutional neural networks. Thus, they return the class and the position of the detected object. Furthermore, Detectron2 is also an instance segmentation algorithm able to return a corresponding mask that deﬁnes the area of the object. YOLOv5 [25] comes from You Only Look Once and claims to be one of the fastest object detection algorithms. In fact, it is one of the best-known object detection algorithms due to its speed and accuracy. It is divided into three components, namely, a backbone, neck and head, as all single-stage detectors. In particular, the ﬁrst component contains a backbone network that extracts rich feature representations from images. The second component consists of the model neck responsible for extracting feature pyramids that help to recognise objects of different sizes and scales. The ﬁnal component is the model head that applies anchor boxes on feature maps. In addition, it is responsible for rendering the predicted class, the scores of the predicted objects, and the bounding boxes. Detectron2 [23] is considered state-of-the-art in instance segmentation. It is the succes- sor of Mask RCNN [30], which is built on top of Faster RCNN. Thus, apart from providing The architecture of Mask RCNN is divided into three stages. In the ﬁrst stage, a regional proposal network (RPN) is responsible for returning all regions of possible areas with detected objects. Mask RCNN uses ResNet50 or ResNet101 with FPN support as a backbone network in this stage. In the second stage, a classiﬁer is responsible for evaluating the region proposals derived from the ﬁrst stage and providing the predictions with bounding boxes for each detected object. Finally, the third stage provides the corresponding masks of the detected objects. ',\n",
       " 'materials and methods',\n",
       " ' This section presents the methodology we followed in our research. First, we describe the image acquisition from mushrooms in the greenhouse and the annotation process. Second, we give the basic conﬁguration of YOLOv5 for mushroom classiﬁcation in three different growing stages. Moreover, we provide the architecture we used for mushroom growth monitoring. Data collection took place in a greenhouse near the city of Grevena, Greece. We collected multiple images in a greenhouse, each containing one or multiple mushrooms. We captured only images that had mushrooms mainly in the foreground and not in the background because they reduced the detection accuracy. Finally, we obtained 1128 images with one or more mushrooms. We annotated them into three different classes (Stage1, Stage2, Stage3) depending on the growing stage. In more detail, the ﬁrst stage corresponded to the ﬁrst days of appearance, when mushrooms were too small and they had just started to form their shape. The second class corresponded to mushrooms that had already formed their shape and continued to grow. Some of them were obviously small, while others were big enough but not ready to harvest. The third class contained mushrooms that were ready to harvest, which could be manually indicated from two parameters. More speciﬁcally, when the edges of their caps started to become ﬂat or slightly uprolled, it was an indication of their ﬁnal stage of growth [31]. Figure 1 shows some examples of mushrooms belonging to the three different stages. Many of the collected photos in the greenhouse contained multiple mushrooms, making the annotation complex and the recognition even more challenging, especially when the mushrooms were at the ﬁnal stage and many of them were overlapping. Figure 2 presents two of those photos as an example. Figure 2. Pictures from the greenhouse with multiple mushrooms. Finally, 4271 mushrooms were annotated and classiﬁed into three different classes (Stage1, Stage2, Stage3) as described before. Moreover, 1130 of them belonged to the ﬁrst growing stage, 1845 to the second growing stage, and 1296 of them to the third growing stage. In addition, the 1128 annotated photos were divided into 784 for the training dataset and 344 for the validation dataset. For the second part of our research, we collected a different set of photos. In particular, the data acquisition for the growing rate of mushrooms was made in 33 different mushroom substrate grow bags. We captured a photo for each one on seven different days between 16 August 2022 and 24 August 2022. Finally, we collected 231 different photos in total. The distance between the camera and the substrate grow bag was one meter above. Figure 3 shows an example of three captured photos. We did not annotate these photos, but we used the trained models from Detectron2 to indicate the size of each mushroom. In our evaluation for mushroom growth monitoring, we used only the mushrooms that were on top of the substrate grow bag. For training Detectron2 for instance segmentation, we annotated another two datasets. The ﬁrst one contained annotations for substrate grow bags, and the second one contained annotations for mushrooms. The annotation process for the image segmentation is a time- consuming task since we had to annotate precisely the objects with polygons. For that reason, we chose to annotate only photos that contained a small number of mushrooms to simplify the annotation. Finally, we annotated 453 mushrooms and divided them into 358 for the training dataset and 95 for the validation dataset. In addition, for the detection of substrate grow bags, we annotated 200 photos and divided them into 150 for the training dataset and 50 for the validation dataset. For mushroom detection and classiﬁcation in the three different classes as described previously, we used YOLOv5. First, we trained our models with the default hyperparame- ters using the pretrained models YOLOv5s and YOLOv5l with an image size of 640 pixels and batch sizes of 2, 4, and 8. To achieve better results, we used the provided method integrated with YOLOv5 for hyperparameter optimisation called hyperparameter evolution. It is a genetic algorithm to ﬁnd the best set of hyperparameters for the speciﬁc dataset. The evolving procedure starts from the default hyperparameters or other user-deﬁned values, if available, and tries to improve a ﬁtness function in every loop. The default ﬁtness function is a weighted combination of mAP@0.5 with 10% contribution and mAP@0.5:0.95 with 90% contribution. In every evolving loop, the primary genetic operator is the mutation. The proposed combination for the mutation uses 80% probability and a 0.04 variance to calculate the next combination of hyperparameters based on the best parents from previous generations. We used the hyperparameter evolution approach for the two pretrained models, YOLOv5s and YOLOv5l, and trained them for 600 generations with an image size of 640 pixels and a batch size of 4. Table 1 shows the predicted sets of hyperparameters for each pretrained model. Table 1. Optimised hyperparameters after evolving for YOLOv5s and YOLOv5l. Figure 4 shows a graphical representation for YOLOv5l with each hyperparameter displayed in a different subplot. Each subplot presents all values from all generations for the speciﬁc hyperparameter. The horizontal axis corresponds to the value of the hyperparameter, and the vertical value corresponds to the calculated ﬁtness. The yellow areas indicate a high concentration of values. Subplots with vertical distributions indicate that the speciﬁc hyperparameter was disabled and did not mutate. Figure 4. Graphical representation for the calculated values of all hyperparameters, after evolving for 600 generations for YOLOv5l. The mAP@0.5 metric corresponds to the average precision over a threshold of 0.5 for the object detection. In addition, the mAP@0.5:0.95 denotes the average precision with a threshold between 0.5 and 0.95. The precision metric indicates the correctly identiﬁed trees divided by the total number of detected trees and is given by Equation (1). The recall metric indicates the falsely identi- ﬁed trees divided by the total number of actual existing trees and is given by Equation (2). where TP (true positive) is the number of correctly detected objects in the class, FP (false positive) is the number of falsely detected objects in the speciﬁc class, and FN (false negative) is the number of objects that are not detected in the speciﬁc class. For mushroom growth monitoring, we used two different stages for the object detec- tion. The ﬁrst one was responsible for detecting the substrate grow bag, and the second one was responsible for detecting the mushrooms in a photo. Figure 5 illustrates the main components of the architecture of the proposed method. More speciﬁcally, in the stage of the substrate grow bag detection, we use the trained model with Detectron2. We only obtained the bounding box of the detected substrate grow bag and not the mask. After the detection of the substrate grow bag, we used the corresponding bounding box and divided it into six equal rectangles (2 × 3). Figure 6a illustrates the bounding box (green) and the lines (red) that divide the bounding box in order to distinguish the detected mushrooms. Moreover, we used the trained model with Detectron2 to detect the mushrooms in the photo and return the corresponding masks. Figure 6b displays the detected mush- rooms. Next, we performed a resize operation on the bounding box and the masks to achieve normalisation for calculating the size in all photos of the same substrate grow bag. Furthermore, the detected mushrooms were distinguished in one of the rectangles with the following rules. A mushroom should have more than 50% in the speciﬁc rectangle. Only one mushroom can belong in one rectangle, and each mushroom belongs only in one rectangle. In any case, the selected mushroom was the one that covered the most area in the rectangle. Finally, Figure 6c illustrates the divided bounding box with the corresponding masks of all detected masks. The size of the provided mask could indicate the size of the mushroom. After distinguishing each mushroom, we calculated the coverage based on the pixels of the provided mask and used it for mushroom growth monitoring. ',\n",
       " 'results',\n",
       " ' In the following subsections, we evaluate the results of the mushroom classiﬁcation for different growth stages. Next, we provide the results of mushroom monitoring for their growth rate. In this subsection, we compared different conﬁgurations of YOLOv5 for mushroom detection and classiﬁcation in three classes, as described previously. We used the metrics mAP@0.5, mAP@0.5:0.95, precision, recall, and F1-score to compare the accuracy of each trained model. Table 2 shows a comparison of all conﬁgurations. More speciﬁcally, we use the pretrained models YOLOv5s and YOLOv5l with the default (D) conﬁguration for the hyperparameters and with the conﬁguration derived from evolution (E). Moreover, our conﬁgurations used an image size of 640 pixels and three different values for the batch size (2, 4, 8). Table 2. Evaluation results for mAP@0.5 and mAP@0.5:0.95 for different hyperparameter conﬁgura- tions (HC), for default values (D) and optimised values with evolution (E). It seems that there was not much difference in all conﬁgurations. Based on mAP@50 and mAP@50:95, conﬁgurations with hyperparameters derived from evolution showed a slightly better performance. Moreover, conﬁgurations based on YOLOv5l returned a slightly better performance. The model based on YOLOv5l with hyperparameters de- rived from evolution and with a batch size of eight achieved the best performance. More speciﬁcally, it reached an mAP@0.5 of 0.79353 and an mAP@0.5:0.95 of 0.54582. Another type of evaluation was based on the F1-Score metric. Table 3 shows the comparison of all conﬁgurations based on the best occurrence of the F1-Score. Better performance on this metric was achieved by the same conﬁguration with YOLOv5l, hy- perparameters derived from evolution, and a batch size of eight, as it reached an F1-Score of 76.65%. Table 3. Evaluation results for the F1-Score for different hyperparameter conﬁgurations (HC), for default values (D) and optimised values with evolution (E). Figure 7 shows the curve of the F1-Score for the best conﬁguration. The horizontal axis corresponds to the conﬁdence of the detected mushrooms, while the vertical axis corresponds to the F1-score. In this example, the maximum F1-score had a conﬁdence of 0.566. In addition, the graph shows that the class “Stage1” had a better accuracy. This was expected since all photos with mushrooms belonging to “Stage1” were simpler, as mushrooms were small and not overlapping. To notice the detection accuracy for each class more clearly, Figure 8 presents the confusion matrix for the same conﬁguration. It shows that class “Stage1” had an accuracy of 82%, class “Stage2” had an accuracy of 71%, and “Stage3” had an accuracy of 70%. Although this does not seem perfect, we can say that it was good enough depending on the complexity of the photos with mushrooms in “Stage2” and “Stage3”. Figure 9 shows an example of the detection where only mushrooms from classes Stage2 and Stage3 exist. The green bounding box indicates that the mushroom is ready to harvest (Stage3). The blue bounding box indicates that the mushroom is not yet ready to harvest (Stage2). We marked in magenta the mushrooms that were detected in the wrong class. Moreover, we marked in red the mushrooms that were not detected. It seems from the photo that the mushrooms in the foreground were detected in the right class, while some errors occurred in mushrooms that were in the background or partially displayed. After the procedure described previously, we obtained 575 different masks from 114 different mushrooms in the greenhouse. All masks were distinguished for different mushrooms depending on the number of the substrate grow bag and the position on it. When the mask corresponded to the ﬁrst day of appearance of each mushroom, it was marked as day one. The corresponding masks of the next days of the same mushroom were numbered as well in the same way. Figure 10 shows the growth rate of four different mushrooms. Each mushroom is indicated with a different colour. The horizontal axis of the graph contains the number of the growth day, starting from number one for the ﬁrst day of the mushroom’s appearance. The vertical axis indicates the size of the mushroom counted in pixels. Moreover, we calculated the maximum size of each mushroom based on all photos before it was harvested. We set its size as 100%, and we calculated the size of the same mushroom for all other days accordingly. Figure 11 shows the growth rate of the same mushrooms but as a percentage of the maximum size of each mushroom. Both graphs show that the three mushrooms (M1, M2, M3) followed an almost linear growth rate. The fourth mushroom (M4) also followed a linear growth rate for the ﬁrst four days, but from the ﬁfth day, it seemed not to change in size. In fact, on the ﬁfth day, that speciﬁc mushroom should have been harvested, but it was accidentally forgotten. Figure 12 shows the captured photos of this mushroom (M4), from which it seemed that the mushroom was ready to harvest on day ﬁve. Figure 12. Mushroom growth example photos. We manually evaluated the harvesting status of each mushroom and decided that 17 mushrooms could be harvested earlier, which represented 14.04% of the mushrooms. The average difference between the size of the masks from the day identiﬁed as ready to harvest and the previous day was 5.34%. Thus, if we set it as a threshold in our method to provide an alert any time that the difference in the mushroom size was below it, we could inform the farmer of the harvest time. This would lead to an improvement in the quality of the harvested mushrooms as we would avoid overripe mushrooms. In addition, our evaluation showed that the average days for the growth of mushrooms was 5.22. Figure 13 displays the graph with all mushrooms detected for all days. All values are displayed in percentage of the maximum size of the speciﬁc mushroom. The blue marks indicate the detected mushrooms that were not ready to harvest, while the red marks indicate mushrooms that should have been harvested at least one day before. ',\n",
       " 'discussion',\n",
       " ' Our ﬁrst approach for mushroom detection with YOLOv5 could be a valuable part of a decision support system. For example, it may be useful in tasks such as yield prediction or product improvement by indicating those mushrooms that are ready for harvest. Overall, the accuracy of the trained model was considered acceptable regarding the complexity of the photos of the dataset. However, our experience shows that some changes could improve the results. A more precisely selected dataset should improve the detection accuracy of different classes. For example, photos with fewer mushrooms and only in the foreground will give more accurate results. As we discussed in Figure 9, the main drawback in detection accuracy is that many mushrooms are partially visible in the photos, and many of them are in the background, which makes it difﬁcult to detect and classify because their features are not clear. Moreover, a proposed methodology to overcome these obstacles could be to exclude mushrooms that are partially displayed in the picture and to focus only on those clearly displayed. Furthermore, the 600 steps we used for the hyperparameter evolution in this research may not be good enough, and more steps will result in a better detection accuracy. This would lead to better combinations of the hyperparameters, but the hyperparameter evolu- tion procedure is a very time-consuming task, even for a professional graphic processing unit. A better approach to enhance the model’s accuracy is to modify the YOLOv5 algorithm. Thus, the trained model would include the speciﬁc characteristics of the different classes. Our second approach for mushroom growth monitoring could also be helpful for a decision support system to inform farmers about the harvesting time. As a result, an improvement in product quality is expected since mushrooms will be collected at the right time. Furthermore, the precisely detected mask of the mushrooms may support other potential applications. For example, it could be a useful part of a robotic mechanism in order to detect, decide, and collect only those mushrooms that are ready to harvest without damaging them. The method of mushroom growth monitoring with Detectron2 also has some limi- tations. Our method focused only on mushrooms that were on top of the substrate bag, which was placed horizontally in the greenhouse. In fact, only one-third of the mushrooms could be observed with this method since there were also substrate bags positioned verti- cally below the substrate bag we used. In addition, in many cases, the substrate bags are densely placed in the greenhouse, and sometimes, the farmers use more than one layer of substrate grow bags in the greenhouse, making it difﬁcult to capture photos even from those positioned horizontally. Furthermore, getting only one image per day has the disadvantage that the growing days may not be counted precisely in some mushrooms. For example, some mushrooms may appear a few hours after a picture was taken, resulting in a day count less than the actual growth duration. Moreover, some mushrooms may have been harvested just before a picture was taken, also resulting in reduced count days. Therefore, capturing photos of the substrate bags more often could give better results and inform more precisely about the growing rate of the mushroom and the harvesting time. Finally, the results of the mushroom detection are not always precise. In fact, the proposed method has two minor drawbacks. First, some of the mushrooms that were too small could not be detected. In addition, the masks returned from Detectron2 were not always accurate. We believe that both these drawbacks do not have a large impact on the proposed methodology. However, if we need to improve them, we could use a larger dataset with more annotated mushrooms to get better results. ',\n",
       " 'conclusions',\n",
       " ' Object detection and instance segmentation have multiple applications in various domains. In smart farming, it is mainly used in fruit detection, weed detection, or pest detection. In this paper, we evaluated the effectiveness of YOLOv5 and Detectron2 in mushroom detection in a greenhouse. Firstly, we evaluated YOLOv5 on its effectiveness in classifying mushrooms in three growth stages. The results showed that even in complex environments such as a greenhouse with Pleurotus mushrooms, it was possible to identify mushrooms that were ready to harvest. The evaluation results on mushroom detection and classiﬁcation in three different growing stages gave an F1-score of up to 76.5%, and speciﬁcally for the ﬁnal growing stage, an accuracy of up to 70%. Secondly, we proposed a method for mushroom growth monitoring. For that purpose, we used two trained models with Detectron2. The ﬁrst one detected the substrate grow bag, and the second detected the mushrooms and returned the corresponding masks. Experi- mental results showed that the growth rate of Pleurotus mushrooms was linear. Moreover, this procedure made it possible to detect when mushrooms reached their maximum size and were ready to harvest. The evaluation results showed that it was possible to make de- cisions and improve harvesting time for up to 14.04% of the mushrooms in the greenhouse. In addition, the results showed that, on average, Pleurotus mushrooms needed 5.22 days to reach maximum size. Moreover, the proposed methods are suitable to be part of a decision support system to inform farmers about the status of their cultivation. In addition, a future potential application would be a robotic mechanism able to detect and recognise the growth stage of mushrooms before harvesting them. Author Contributions: Conceptualization, V.M. and P.S.; methodology, V.M. and G.K.; software, V.M.; validation, G.K. and P.S.; formal analysis, V.M.; investigation, V.M.; resources, V.M. and N.M.; data curation, V.M. and G.K.; writing—original draft preparation, V.M.; writing—review and editing, S.B. and I.M.; visualization, V.M.; supervision, P.S.; project administration, P.S.; funding acquisition, P.S. All authors have read and agreed to the published version of the manuscript. Funding: This research was cofunded by the European Regional Development Fund of the European Union and Greek national funds through the Operational Program Western Macedonia 2014–2020, under the call “Collaborative and networking actions between research institutions, educational institutions and companies in priority areas of the strategic smart specialization plan of the region”, project “Smart Mushroom fARming with internet of Things—SMART”, project code: DMR-0016521. Data Availability Statement: The data presented in this study are available on request from the corresponding author. The data are not publicly available due to restrictions on privacy. ']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts_with_chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab484fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Citation: Moysiadis, V.; Kokkonis, G.; Bibi, S.; Moscholios, I.; Maropoulos, N.; Sarigiannidis, P. Monitoring Mushroom Growth with Machine Learning. Agriculture 2023, 13, 223. https://doi.org/10.3390/ agriculture13010223 Copyright: © 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). Mushrooms are a rich source of nutrients, proteins, minerals, and vitamins (B, C, and D) [1]. More than 200 species of edible mushrooms are used as ingredients in traditional foods around the world [2], but only 35 species are cultivated in greenhouses in restricted environments [3]. The mushrooms of the genus Pleurotus are in second place worldwide in the industry. Therefore, improving the production line will have a major impact on the economy of the speciﬁc domain. The authors in [4] presented a comprehensive review of the factors affecting the mushrooms of the genus Pleurotus spp. In addition, many research efforts have focused on solutions based on modern technologies from the Internet of things, aiming to transform traditional farming into the new era called smart farming [5]. This transition brings many applications in various agriculture tasks aiming to increase production, reduce cost production, reduce chemical inputs, and reduce labour effort. Along with other technologies, machine learning is highly used in tasks such as yield prediction, disease detection, weed recognition, and fruit recognition. More speciﬁcally, many research efforts try to classify and detect the speciﬁc location of different objects in images. For example, a robotic mechanism is suitable for weeding if it has the ability to recognise weeds from vegetation [6]. The detection of fruits with machine learning is also valuable in robotic mechanisms [7–9]. Disease detection has already seen numerous research efforts evaluating the ability to detect various diseases in various crops [10–12]. Moreover, insect detection is another important task in cultivation. Thus, many research efforts exist in this area [13,14]. Furthermore, machine learning is already used in research efforts in mushroom culti- vation or wild mushroom hunting, aiming to give solutions to various tasks. The authors in [15] gave a comprehensive review in this area. One of the main tasks based on machine learning focuses on species recognition. For example, the authors in [16] provided a method to distinguish edible mushrooms from poisonous ones. In addition, the authors in [17] suggested a method for species recognition with a mobile smart device. Another research effort on mushroom species classiﬁcation is presented in [18]. Furthermore, a few research efforts have used object detection algorithms in order to localise and classify mushrooms in a photo. The authors in [19] developed a system that used object detection to localise and recognise mushrooms that were ready to harvest. In [20], the authors evaluated the poten- tial of mushroom hunting using a custom-made unmanned aerial vehicle (UAV). Finally, the authors in [21] presented an automatic robotic mechanism for mushroom harvesting using machine learning for detection. Object detection algorithms have gained signiﬁcant momentum in recent years due to the success of deep learning [22], a speciﬁc area of machine learning. As a result, many object detection algorithms have been available in recent years that can detect and provide the exact location of an object in an image or video. In addition, some of them are also capable of providing a corresponding mask of the object. This category is called instance segmentation. Detectron2 [23] is the most popular in this category, while some other variants, such as Mask Scoring RCNN [24], try to return better results of the provided masks. In addition, YOLOv5 [25] is one of the most popular object detection algorithms, delivering accurate and rapid results. With recent technologies, a sufﬁcient number of research efforts are available for mush- room cultivation. Most of them propose controlling growing conditions in the greenhouse and keeping them within speciﬁc boundaries [26,27]. However, only a few of the research efforts go a step over and use more advanced mushroom cultivation methods. Machine learning promise to provide efﬁcient methods in this direction. Thus, we should use it to help farmers in their everyday activities and support their decisions. Until now, only a few research efforts have used machine learning in the mushroom industry. As described before, most of them deal with mushroom species classiﬁcation, and only a few try to give solutions for usable tasks such as mushroom harvesting in the greenhouse. Moreover, none of the existing methods for mushroom harvesting utilises the genus Pleurotus. Our work uses machine learning not only to detect the mushrooms in the greenhouse but also to classify them in three different stages depending on their growth status and predict when they are ready to harvest. For this purpose, we use the YOLOv5 [25] object detection algorithm and evaluate its accuracy on mushroom detection with different con- ﬁgurations of hyperparameters. The evaluation results show that YOLOv5 can detect and classify mushrooms with an F1-score of up to 76.5% and detect the ﬁnal stage of mushroom growth with an accuracy of up to 70%. Furthermore, in the second part of our research, we use Detectron2 [23] to extract a corresponding mask for each mushroom and use it to calculate its size and monitor the growth rate. The results show that they follow a linear growth rate, and it is also possible to predict the harvesting day based on images on previous days. In addition, the method can improve the harvesting time for 14.04% of the mushrooms. Our work could be a valuable method for mushroom detection on greenhouses with practical applications. For example, it could be used for yield prediction, where ground cameras can observe the greenhouse and predict the yield a few days before the mushrooms are ready to harvest. Another possible application could be on robotics mechanisms that can detect the exact crown of the mushrooms in order to harvest them with no damage. This work is part of our decision support system described in our previous work [28], where we presented a system architecture that covered mushroom cultivation in a greenhouse and the aggregation of useful information for wild mushroom hunting. The rest of this paper is as follows. In Section 2, we give the most relevant research efforts for mushroom detection using machine learning or image processing. Section 3 brieﬂy discusses the basic concepts of object detection and instance segmentation and provides the essential features of YOLOv5 and Detectron2. Section 4 analyses the method- ology we follow in this research. Next, Section 5 presents the evaluation results between different conﬁgurations of YOLOv5 and its effectiveness on mushroom classiﬁcation at different stages of mushroom growth. In addition, we provide the results from Detectron2 on mushroom growth monitoring. Section 6 discusses the effectiveness of the proposed methods, their limitations and possible impact on future applications. Finally, Section 7 concludes this paper. ',\n",
       " ' In this section, we brieﬂy present similar research efforts in the area of smart farming, particularly in image processing or machine learning for image segmentation or object recognition on mushroom cultivation. A harvesting robot for oyster mushrooms was proposed in [21]. The development system used an improved SSD algorithm to detect mushrooms ready to harvest. The algorithm used RGB images and point clouds collected by an Intel RealSense D435i camera. The evaluation results showed an accuracy of mushroom recognition of up to 95.0% and a harvesting success rate of up to 86.8%. The average harvesting time for a single mushroom was 8.85 s. The authors in [19] provided a measurement monitoring system to observe mushroom growth in a greenhouse. The proposed system used YOLOv3 for mushroom detection and an additional localisation method to improve the position of the detected mushroom in order to distinguish the same mushroom in different captured photos. Moreover, the system was able to estimate the harvesting time. In [16], the authors presented a mushroom farm automation to classify toxic mush- rooms. The proposed system adopted machine learning to distinguish edible mushrooms from poisonous mushrooms. The utilised model was a combination of six different clas- siﬁers that worked together and reach an accuracy of 100%. More speciﬁcally, the en- semble model used the following classiﬁers: decision tree (DT), logistic regression (LR), K-nearest neighbours (KNN), support vector machine (SVM), naive Bayes (NB), and ran- dom forest (RF). Moreover, the manuscript introduced an architectural design for smart mushroom farming. The authors in [29] presented an automatic sorting system for fresh, white button mushrooms. Apart from the automatic mechanism, they proposed an image processing algorithm to collect button mushrooms. The algorithm eliminated the shadow and petiole on the image and determined the pileus diameter of the mushrooms. Experimental results showed that compared to manual grading speed, their approach was improved by 38.86%, and the accuracy was improved by 6.84%. ',\n",
       " ' In this section, we brieﬂy presenting the main characteristics of YOLOv5 [25] and Detectron2 [23]. Both of them are suitable for object detection in images or videos. Moreover, they both belong to object detection algorithms based on convolutional neural networks. Thus, they return the class and the position of the detected object. Furthermore, Detectron2 is also an instance segmentation algorithm able to return a corresponding mask that deﬁnes the area of the object. YOLOv5 [25] comes from You Only Look Once and claims to be one of the fastest object detection algorithms. In fact, it is one of the best-known object detection algorithms due to its speed and accuracy. It is divided into three components, namely, a backbone, neck and head, as all single-stage detectors. In particular, the ﬁrst component contains a backbone network that extracts rich feature representations from images. The second component consists of the model neck responsible for extracting feature pyramids that help to recognise objects of different sizes and scales. The ﬁnal component is the model head that applies anchor boxes on feature maps. In addition, it is responsible for rendering the predicted class, the scores of the predicted objects, and the bounding boxes. Detectron2 [23] is considered state-of-the-art in instance segmentation. It is the succes- sor of Mask RCNN [30], which is built on top of Faster RCNN. Thus, apart from providing The architecture of Mask RCNN is divided into three stages. In the ﬁrst stage, a regional proposal network (RPN) is responsible for returning all regions of possible areas with detected objects. Mask RCNN uses ResNet50 or ResNet101 with FPN support as a backbone network in this stage. In the second stage, a classiﬁer is responsible for evaluating the region proposals derived from the ﬁrst stage and providing the predictions with bounding boxes for each detected object. Finally, the third stage provides the corresponding masks of the detected objects. ',\n",
       " ' This section presents the methodology we followed in our research. First, we describe the image acquisition from mushrooms in the greenhouse and the annotation process. Second, we give the basic conﬁguration of YOLOv5 for mushroom classiﬁcation in three different growing stages. Moreover, we provide the architecture we used for mushroom growth monitoring. Data collection took place in a greenhouse near the city of Grevena, Greece. We collected multiple images in a greenhouse, each containing one or multiple mushrooms. We captured only images that had mushrooms mainly in the foreground and not in the background because they reduced the detection accuracy. Finally, we obtained 1128 images with one or more mushrooms. We annotated them into three different classes (Stage1, Stage2, Stage3) depending on the growing stage. In more detail, the ﬁrst stage corresponded to the ﬁrst days of appearance, when mushrooms were too small and they had just started to form their shape. The second class corresponded to mushrooms that had already formed their shape and continued to grow. Some of them were obviously small, while others were big enough but not ready to harvest. The third class contained mushrooms that were ready to harvest, which could be manually indicated from two parameters. More speciﬁcally, when the edges of their caps started to become ﬂat or slightly uprolled, it was an indication of their ﬁnal stage of growth [31]. Figure 1 shows some examples of mushrooms belonging to the three different stages. Many of the collected photos in the greenhouse contained multiple mushrooms, making the annotation complex and the recognition even more challenging, especially when the mushrooms were at the ﬁnal stage and many of them were overlapping. Figure 2 presents two of those photos as an example. Figure 2. Pictures from the greenhouse with multiple mushrooms. Finally, 4271 mushrooms were annotated and classiﬁed into three different classes (Stage1, Stage2, Stage3) as described before. Moreover, 1130 of them belonged to the ﬁrst growing stage, 1845 to the second growing stage, and 1296 of them to the third growing stage. In addition, the 1128 annotated photos were divided into 784 for the training dataset and 344 for the validation dataset. For the second part of our research, we collected a different set of photos. In particular, the data acquisition for the growing rate of mushrooms was made in 33 different mushroom substrate grow bags. We captured a photo for each one on seven different days between 16 August 2022 and 24 August 2022. Finally, we collected 231 different photos in total. The distance between the camera and the substrate grow bag was one meter above. Figure 3 shows an example of three captured photos. We did not annotate these photos, but we used the trained models from Detectron2 to indicate the size of each mushroom. In our evaluation for mushroom growth monitoring, we used only the mushrooms that were on top of the substrate grow bag. For training Detectron2 for instance segmentation, we annotated another two datasets. The ﬁrst one contained annotations for substrate grow bags, and the second one contained annotations for mushrooms. The annotation process for the image segmentation is a time- consuming task since we had to annotate precisely the objects with polygons. For that reason, we chose to annotate only photos that contained a small number of mushrooms to simplify the annotation. Finally, we annotated 453 mushrooms and divided them into 358 for the training dataset and 95 for the validation dataset. In addition, for the detection of substrate grow bags, we annotated 200 photos and divided them into 150 for the training dataset and 50 for the validation dataset. For mushroom detection and classiﬁcation in the three different classes as described previously, we used YOLOv5. First, we trained our models with the default hyperparame- ters using the pretrained models YOLOv5s and YOLOv5l with an image size of 640 pixels and batch sizes of 2, 4, and 8. To achieve better results, we used the provided method integrated with YOLOv5 for hyperparameter optimisation called hyperparameter evolution. It is a genetic algorithm to ﬁnd the best set of hyperparameters for the speciﬁc dataset. The evolving procedure starts from the default hyperparameters or other user-deﬁned values, if available, and tries to improve a ﬁtness function in every loop. The default ﬁtness function is a weighted combination of mAP@0.5 with 10% contribution and mAP@0.5:0.95 with 90% contribution. In every evolving loop, the primary genetic operator is the mutation. The proposed combination for the mutation uses 80% probability and a 0.04 variance to calculate the next combination of hyperparameters based on the best parents from previous generations. We used the hyperparameter evolution approach for the two pretrained models, YOLOv5s and YOLOv5l, and trained them for 600 generations with an image size of 640 pixels and a batch size of 4. Table 1 shows the predicted sets of hyperparameters for each pretrained model. Table 1. Optimised hyperparameters after evolving for YOLOv5s and YOLOv5l. Figure 4 shows a graphical representation for YOLOv5l with each hyperparameter displayed in a different subplot. Each subplot presents all values from all generations for the speciﬁc hyperparameter. The horizontal axis corresponds to the value of the hyperparameter, and the vertical value corresponds to the calculated ﬁtness. The yellow areas indicate a high concentration of values. Subplots with vertical distributions indicate that the speciﬁc hyperparameter was disabled and did not mutate. Figure 4. Graphical representation for the calculated values of all hyperparameters, after evolving for 600 generations for YOLOv5l. The mAP@0.5 metric corresponds to the average precision over a threshold of 0.5 for the object detection. In addition, the mAP@0.5:0.95 denotes the average precision with a threshold between 0.5 and 0.95. The precision metric indicates the correctly identiﬁed trees divided by the total number of detected trees and is given by Equation (1). The recall metric indicates the falsely identi- ﬁed trees divided by the total number of actual existing trees and is given by Equation (2). where TP (true positive) is the number of correctly detected objects in the class, FP (false positive) is the number of falsely detected objects in the speciﬁc class, and FN (false negative) is the number of objects that are not detected in the speciﬁc class. For mushroom growth monitoring, we used two different stages for the object detec- tion. The ﬁrst one was responsible for detecting the substrate grow bag, and the second one was responsible for detecting the mushrooms in a photo. Figure 5 illustrates the main components of the architecture of the proposed method. More speciﬁcally, in the stage of the substrate grow bag detection, we use the trained model with Detectron2. We only obtained the bounding box of the detected substrate grow bag and not the mask. After the detection of the substrate grow bag, we used the corresponding bounding box and divided it into six equal rectangles (2 × 3). Figure 6a illustrates the bounding box (green) and the lines (red) that divide the bounding box in order to distinguish the detected mushrooms. Moreover, we used the trained model with Detectron2 to detect the mushrooms in the photo and return the corresponding masks. Figure 6b displays the detected mush- rooms. Next, we performed a resize operation on the bounding box and the masks to achieve normalisation for calculating the size in all photos of the same substrate grow bag. Furthermore, the detected mushrooms were distinguished in one of the rectangles with the following rules. A mushroom should have more than 50% in the speciﬁc rectangle. Only one mushroom can belong in one rectangle, and each mushroom belongs only in one rectangle. In any case, the selected mushroom was the one that covered the most area in the rectangle. Finally, Figure 6c illustrates the divided bounding box with the corresponding masks of all detected masks. The size of the provided mask could indicate the size of the mushroom. After distinguishing each mushroom, we calculated the coverage based on the pixels of the provided mask and used it for mushroom growth monitoring. ',\n",
       " ' In the following subsections, we evaluate the results of the mushroom classiﬁcation for different growth stages. Next, we provide the results of mushroom monitoring for their growth rate. In this subsection, we compared different conﬁgurations of YOLOv5 for mushroom detection and classiﬁcation in three classes, as described previously. We used the metrics mAP@0.5, mAP@0.5:0.95, precision, recall, and F1-score to compare the accuracy of each trained model. Table 2 shows a comparison of all conﬁgurations. More speciﬁcally, we use the pretrained models YOLOv5s and YOLOv5l with the default (D) conﬁguration for the hyperparameters and with the conﬁguration derived from evolution (E). Moreover, our conﬁgurations used an image size of 640 pixels and three different values for the batch size (2, 4, 8). Table 2. Evaluation results for mAP@0.5 and mAP@0.5:0.95 for different hyperparameter conﬁgura- tions (HC), for default values (D) and optimised values with evolution (E). It seems that there was not much difference in all conﬁgurations. Based on mAP@50 and mAP@50:95, conﬁgurations with hyperparameters derived from evolution showed a slightly better performance. Moreover, conﬁgurations based on YOLOv5l returned a slightly better performance. The model based on YOLOv5l with hyperparameters de- rived from evolution and with a batch size of eight achieved the best performance. More speciﬁcally, it reached an mAP@0.5 of 0.79353 and an mAP@0.5:0.95 of 0.54582. Another type of evaluation was based on the F1-Score metric. Table 3 shows the comparison of all conﬁgurations based on the best occurrence of the F1-Score. Better performance on this metric was achieved by the same conﬁguration with YOLOv5l, hy- perparameters derived from evolution, and a batch size of eight, as it reached an F1-Score of 76.65%. Table 3. Evaluation results for the F1-Score for different hyperparameter conﬁgurations (HC), for default values (D) and optimised values with evolution (E). Figure 7 shows the curve of the F1-Score for the best conﬁguration. The horizontal axis corresponds to the conﬁdence of the detected mushrooms, while the vertical axis corresponds to the F1-score. In this example, the maximum F1-score had a conﬁdence of 0.566. In addition, the graph shows that the class “Stage1” had a better accuracy. This was expected since all photos with mushrooms belonging to “Stage1” were simpler, as mushrooms were small and not overlapping. To notice the detection accuracy for each class more clearly, Figure 8 presents the confusion matrix for the same conﬁguration. It shows that class “Stage1” had an accuracy of 82%, class “Stage2” had an accuracy of 71%, and “Stage3” had an accuracy of 70%. Although this does not seem perfect, we can say that it was good enough depending on the complexity of the photos with mushrooms in “Stage2” and “Stage3”. Figure 9 shows an example of the detection where only mushrooms from classes Stage2 and Stage3 exist. The green bounding box indicates that the mushroom is ready to harvest (Stage3). The blue bounding box indicates that the mushroom is not yet ready to harvest (Stage2). We marked in magenta the mushrooms that were detected in the wrong class. Moreover, we marked in red the mushrooms that were not detected. It seems from the photo that the mushrooms in the foreground were detected in the right class, while some errors occurred in mushrooms that were in the background or partially displayed. After the procedure described previously, we obtained 575 different masks from 114 different mushrooms in the greenhouse. All masks were distinguished for different mushrooms depending on the number of the substrate grow bag and the position on it. When the mask corresponded to the ﬁrst day of appearance of each mushroom, it was marked as day one. The corresponding masks of the next days of the same mushroom were numbered as well in the same way. Figure 10 shows the growth rate of four different mushrooms. Each mushroom is indicated with a different colour. The horizontal axis of the graph contains the number of the growth day, starting from number one for the ﬁrst day of the mushroom’s appearance. The vertical axis indicates the size of the mushroom counted in pixels. Moreover, we calculated the maximum size of each mushroom based on all photos before it was harvested. We set its size as 100%, and we calculated the size of the same mushroom for all other days accordingly. Figure 11 shows the growth rate of the same mushrooms but as a percentage of the maximum size of each mushroom. Both graphs show that the three mushrooms (M1, M2, M3) followed an almost linear growth rate. The fourth mushroom (M4) also followed a linear growth rate for the ﬁrst four days, but from the ﬁfth day, it seemed not to change in size. In fact, on the ﬁfth day, that speciﬁc mushroom should have been harvested, but it was accidentally forgotten. Figure 12 shows the captured photos of this mushroom (M4), from which it seemed that the mushroom was ready to harvest on day ﬁve. Figure 12. Mushroom growth example photos. We manually evaluated the harvesting status of each mushroom and decided that 17 mushrooms could be harvested earlier, which represented 14.04% of the mushrooms. The average difference between the size of the masks from the day identiﬁed as ready to harvest and the previous day was 5.34%. Thus, if we set it as a threshold in our method to provide an alert any time that the difference in the mushroom size was below it, we could inform the farmer of the harvest time. This would lead to an improvement in the quality of the harvested mushrooms as we would avoid overripe mushrooms. In addition, our evaluation showed that the average days for the growth of mushrooms was 5.22. Figure 13 displays the graph with all mushrooms detected for all days. All values are displayed in percentage of the maximum size of the speciﬁc mushroom. The blue marks indicate the detected mushrooms that were not ready to harvest, while the red marks indicate mushrooms that should have been harvested at least one day before. ',\n",
       " ' Our ﬁrst approach for mushroom detection with YOLOv5 could be a valuable part of a decision support system. For example, it may be useful in tasks such as yield prediction or product improvement by indicating those mushrooms that are ready for harvest. Overall, the accuracy of the trained model was considered acceptable regarding the complexity of the photos of the dataset. However, our experience shows that some changes could improve the results. A more precisely selected dataset should improve the detection accuracy of different classes. For example, photos with fewer mushrooms and only in the foreground will give more accurate results. As we discussed in Figure 9, the main drawback in detection accuracy is that many mushrooms are partially visible in the photos, and many of them are in the background, which makes it difﬁcult to detect and classify because their features are not clear. Moreover, a proposed methodology to overcome these obstacles could be to exclude mushrooms that are partially displayed in the picture and to focus only on those clearly displayed. Furthermore, the 600 steps we used for the hyperparameter evolution in this research may not be good enough, and more steps will result in a better detection accuracy. This would lead to better combinations of the hyperparameters, but the hyperparameter evolu- tion procedure is a very time-consuming task, even for a professional graphic processing unit. A better approach to enhance the model’s accuracy is to modify the YOLOv5 algorithm. Thus, the trained model would include the speciﬁc characteristics of the different classes. Our second approach for mushroom growth monitoring could also be helpful for a decision support system to inform farmers about the harvesting time. As a result, an improvement in product quality is expected since mushrooms will be collected at the right time. Furthermore, the precisely detected mask of the mushrooms may support other potential applications. For example, it could be a useful part of a robotic mechanism in order to detect, decide, and collect only those mushrooms that are ready to harvest without damaging them. The method of mushroom growth monitoring with Detectron2 also has some limi- tations. Our method focused only on mushrooms that were on top of the substrate bag, which was placed horizontally in the greenhouse. In fact, only one-third of the mushrooms could be observed with this method since there were also substrate bags positioned verti- cally below the substrate bag we used. In addition, in many cases, the substrate bags are densely placed in the greenhouse, and sometimes, the farmers use more than one layer of substrate grow bags in the greenhouse, making it difﬁcult to capture photos even from those positioned horizontally. Furthermore, getting only one image per day has the disadvantage that the growing days may not be counted precisely in some mushrooms. For example, some mushrooms may appear a few hours after a picture was taken, resulting in a day count less than the actual growth duration. Moreover, some mushrooms may have been harvested just before a picture was taken, also resulting in reduced count days. Therefore, capturing photos of the substrate bags more often could give better results and inform more precisely about the growing rate of the mushroom and the harvesting time. Finally, the results of the mushroom detection are not always precise. In fact, the proposed method has two minor drawbacks. First, some of the mushrooms that were too small could not be detected. In addition, the masks returned from Detectron2 were not always accurate. We believe that both these drawbacks do not have a large impact on the proposed methodology. However, if we need to improve them, we could use a larger dataset with more annotated mushrooms to get better results. ',\n",
       " ' Object detection and instance segmentation have multiple applications in various domains. In smart farming, it is mainly used in fruit detection, weed detection, or pest detection. In this paper, we evaluated the effectiveness of YOLOv5 and Detectron2 in mushroom detection in a greenhouse. Firstly, we evaluated YOLOv5 on its effectiveness in classifying mushrooms in three growth stages. The results showed that even in complex environments such as a greenhouse with Pleurotus mushrooms, it was possible to identify mushrooms that were ready to harvest. The evaluation results on mushroom detection and classiﬁcation in three different growing stages gave an F1-score of up to 76.5%, and speciﬁcally for the ﬁnal growing stage, an accuracy of up to 70%. Secondly, we proposed a method for mushroom growth monitoring. For that purpose, we used two trained models with Detectron2. The ﬁrst one detected the substrate grow bag, and the second detected the mushrooms and returned the corresponding masks. Experi- mental results showed that the growth rate of Pleurotus mushrooms was linear. Moreover, this procedure made it possible to detect when mushrooms reached their maximum size and were ready to harvest. The evaluation results showed that it was possible to make de- cisions and improve harvesting time for up to 14.04% of the mushrooms in the greenhouse. In addition, the results showed that, on average, Pleurotus mushrooms needed 5.22 days to reach maximum size. Moreover, the proposed methods are suitable to be part of a decision support system to inform farmers about the status of their cultivation. In addition, a future potential application would be a robotic mechanism able to detect and recognise the growth stage of mushrooms before harvesting them. Author Contributions: Conceptualization, V.M. and P.S.; methodology, V.M. and G.K.; software, V.M.; validation, G.K. and P.S.; formal analysis, V.M.; investigation, V.M.; resources, V.M. and N.M.; data curation, V.M. and G.K.; writing—original draft preparation, V.M.; writing—review and editing, S.B. and I.M.; visualization, V.M.; supervision, P.S.; project administration, P.S.; funding acquisition, P.S. All authors have read and agreed to the published version of the manuscript. Funding: This research was cofunded by the European Regional Development Fund of the European Union and Greek national funds through the Operational Program Western Macedonia 2014–2020, under the call “Collaborative and networking actions between research institutions, educational institutions and companies in priority areas of the strategic smart specialization plan of the region”, project “Smart Mushroom fARming with internet of Things—SMART”, project code: DMR-0016521. Data Availability Statement: The data presented in this study are available on request from the corresponding author. The data are not publicly available due to restrictions on privacy. ']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts_without_chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1e48b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Citation: Moysiadis, V.; Kokkonis, G.; Bibi, S.; Moscholios, I.; Maropoulos, N.; Sarigiannidis, P. Monitoring Mushroom Growth with Machine Learning. Agriculture 2023, 13, 223. https://doi.org/10.3390/ agriculture13010223 Copyright: © 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). Mushrooms are a rich source of nutrients, proteins, minerals, and vitamins (B, C, and D) [1]. More than 200 species of edible mushrooms are used as ingredients in traditional foods around the world [2], but only 35 species are cultivated in greenhouses in restricted environments [3]. The mushrooms of the genus Pleurotus are in second place worldwide in the industry. Therefore, improving the production line will have a major impact on the economy of the speciﬁc domain. The authors in [4] presented a comprehensive review of the factors affecting the mushrooms of the genus Pleurotus spp. In addition, many research efforts have focused on solutions based on modern technologies from the Internet of things, aiming to transform traditional farming into the new era called smart farming [5]. This transition brings many applications in various agriculture tasks aiming to increase production, reduce cost production, reduce chemical inputs, and reduce labour effort. Along with other technologies, machine learning is highly used in tasks such as yield prediction, disease detection, weed recognition, and fruit recognition. More speciﬁcally, many research efforts try to classify and detect the speciﬁc location of different objects in images. For example, a robotic mechanism is suitable for weeding if it has the ability to recognise weeds from vegetation [6]. The detection of fruits with machine learning is also valuable in robotic mechanisms [7–9]. Disease detection has already seen numerous research efforts evaluating the ability to detect various diseases in various crops [10–12]. Moreover, insect detection is another important task in cultivation. Thus, many research efforts exist in this area [13,14]. Furthermore, machine learning is already used in research efforts in mushroom culti- vation or wild mushroom hunting, aiming to give solutions to various tasks. The authors in [15] gave a comprehensive review in this area. One of the main tasks based on machine learning focuses on species recognition. For example, the authors in [16] provided a method to distinguish edible mushrooms from poisonous ones. In addition, the authors in [17] suggested a method for species recognition with a mobile smart device. Another research effort on mushroom species classiﬁcation is presented in [18]. Furthermore, a few research efforts have used object detection algorithms in order to localise and classify mushrooms in a photo. The authors in [19] developed a system that used object detection to localise and recognise mushrooms that were ready to harvest. In [20], the authors evaluated the poten- tial of mushroom hunting using a custom-made unmanned aerial vehicle (UAV). Finally, the authors in [21] presented an automatic robotic mechanism for mushroom harvesting using machine learning for detection. Object detection algorithms have gained signiﬁcant momentum in recent years due to the success of deep learning [22], a speciﬁc area of machine learning. As a result, many object detection algorithms have been available in recent years that can detect and provide the exact location of an object in an image or video. In addition, some of them are also capable of providing a corresponding mask of the object. This category is called instance segmentation. Detectron2 [23] is the most popular in this category, while some other variants, such as Mask Scoring RCNN [24], try to return better results of the provided masks. In addition, YOLOv5 [25] is one of the most popular object detection algorithms, delivering accurate and rapid results. With recent technologies, a sufﬁcient number of research efforts are available for mush- room cultivation. Most of them propose controlling growing conditions in the greenhouse and keeping them within speciﬁc boundaries [26,27]. However, only a few of the research efforts go a step over and use more advanced mushroom cultivation methods. Machine learning promise to provide efﬁcient methods in this direction. Thus, we should use it to help farmers in their everyday activities and support their decisions. Until now, only a few research efforts have used machine learning in the mushroom industry. As described before, most of them deal with mushroom species classiﬁcation, and only a few try to give solutions for usable tasks such as mushroom harvesting in the greenhouse. Moreover, none of the existing methods for mushroom harvesting utilises the genus Pleurotus. Our work uses machine learning not only to detect the mushrooms in the greenhouse but also to classify them in three different stages depending on their growth status and predict when they are ready to harvest. For this purpose, we use the YOLOv5 [25] object detection algorithm and evaluate its accuracy on mushroom detection with different con- ﬁgurations of hyperparameters. The evaluation results show that YOLOv5 can detect and classify mushrooms with an F1-score of up to 76.5% and detect the ﬁnal stage of mushroom growth with an accuracy of up to 70%. Furthermore, in the second part of our research, we use Detectron2 [23] to extract a corresponding mask for each mushroom and use it to calculate its size and monitor the growth rate. The results show that they follow a linear growth rate, and it is also possible to predict the harvesting day based on images on previous days. In addition, the method can improve the harvesting time for 14.04% of the mushrooms. Our work could be a valuable method for mushroom detection on greenhouses with practical applications. For example, it could be used for yield prediction, where ground cameras can observe the greenhouse and predict the yield a few days before the mushrooms are ready to harvest. Another possible application could be on robotics mechanisms that can detect the exact crown of the mushrooms in order to harvest them with no damage. This work is part of our decision support system described in our previous work [28], where we presented a system architecture that covered mushroom cultivation in a greenhouse and the aggregation of useful information for wild mushroom hunting. The rest of this paper is as follows. In Section 2, we give the most relevant research efforts for mushroom detection using machine learning or image processing. Section 3 brieﬂy discusses the basic concepts of object detection and instance segmentation and provides the essential features of YOLOv5 and Detectron2. Section 4 analyses the method- ology we follow in this research. Next, Section 5 presents the evaluation results between different conﬁgurations of YOLOv5 and its effectiveness on mushroom classiﬁcation at different stages of mushroom growth. In addition, we provide the results from Detectron2 on mushroom growth monitoring. Section 6 discusses the effectiveness of the proposed methods, their limitations and possible impact on future applications. Finally, Section 7 concludes this paper. \\n In this section, we brieﬂy present similar research efforts in the area of smart farming, particularly in image processing or machine learning for image segmentation or object recognition on mushroom cultivation. A harvesting robot for oyster mushrooms was proposed in [21]. The development system used an improved SSD algorithm to detect mushrooms ready to harvest. The algorithm used RGB images and point clouds collected by an Intel RealSense D435i camera. The evaluation results showed an accuracy of mushroom recognition of up to 95.0% and a harvesting success rate of up to 86.8%. The average harvesting time for a single mushroom was 8.85 s. The authors in [19] provided a measurement monitoring system to observe mushroom growth in a greenhouse. The proposed system used YOLOv3 for mushroom detection and an additional localisation method to improve the position of the detected mushroom in order to distinguish the same mushroom in different captured photos. Moreover, the system was able to estimate the harvesting time. In [16], the authors presented a mushroom farm automation to classify toxic mush- rooms. The proposed system adopted machine learning to distinguish edible mushrooms from poisonous mushrooms. The utilised model was a combination of six different clas- siﬁers that worked together and reach an accuracy of 100%. More speciﬁcally, the en- semble model used the following classiﬁers: decision tree (DT), logistic regression (LR), K-nearest neighbours (KNN), support vector machine (SVM), naive Bayes (NB), and ran- dom forest (RF). Moreover, the manuscript introduced an architectural design for smart mushroom farming. The authors in [29] presented an automatic sorting system for fresh, white button mushrooms. Apart from the automatic mechanism, they proposed an image processing algorithm to collect button mushrooms. The algorithm eliminated the shadow and petiole on the image and determined the pileus diameter of the mushrooms. Experimental results showed that compared to manual grading speed, their approach was improved by 38.86%, and the accuracy was improved by 6.84%. \\n In this section, we brieﬂy presenting the main characteristics of YOLOv5 [25] and Detectron2 [23]. Both of them are suitable for object detection in images or videos. Moreover, they both belong to object detection algorithms based on convolutional neural networks. Thus, they return the class and the position of the detected object. Furthermore, Detectron2 is also an instance segmentation algorithm able to return a corresponding mask that deﬁnes the area of the object. YOLOv5 [25] comes from You Only Look Once and claims to be one of the fastest object detection algorithms. In fact, it is one of the best-known object detection algorithms due to its speed and accuracy. It is divided into three components, namely, a backbone, neck and head, as all single-stage detectors. In particular, the ﬁrst component contains a backbone network that extracts rich feature representations from images. The second component consists of the model neck responsible for extracting feature pyramids that help to recognise objects of different sizes and scales. The ﬁnal component is the model head that applies anchor boxes on feature maps. In addition, it is responsible for rendering the predicted class, the scores of the predicted objects, and the bounding boxes. Detectron2 [23] is considered state-of-the-art in instance segmentation. It is the succes- sor of Mask RCNN [30], which is built on top of Faster RCNN. Thus, apart from providing The architecture of Mask RCNN is divided into three stages. In the ﬁrst stage, a regional proposal network (RPN) is responsible for returning all regions of possible areas with detected objects. Mask RCNN uses ResNet50 or ResNet101 with FPN support as a backbone network in this stage. In the second stage, a classiﬁer is responsible for evaluating the region proposals derived from the ﬁrst stage and providing the predictions with bounding boxes for each detected object. Finally, the third stage provides the corresponding masks of the detected objects. \\n This section presents the methodology we followed in our research. First, we describe the image acquisition from mushrooms in the greenhouse and the annotation process. Second, we give the basic conﬁguration of YOLOv5 for mushroom classiﬁcation in three different growing stages. Moreover, we provide the architecture we used for mushroom growth monitoring. Data collection took place in a greenhouse near the city of Grevena, Greece. We collected multiple images in a greenhouse, each containing one or multiple mushrooms. We captured only images that had mushrooms mainly in the foreground and not in the background because they reduced the detection accuracy. Finally, we obtained 1128 images with one or more mushrooms. We annotated them into three different classes (Stage1, Stage2, Stage3) depending on the growing stage. In more detail, the ﬁrst stage corresponded to the ﬁrst days of appearance, when mushrooms were too small and they had just started to form their shape. The second class corresponded to mushrooms that had already formed their shape and continued to grow. Some of them were obviously small, while others were big enough but not ready to harvest. The third class contained mushrooms that were ready to harvest, which could be manually indicated from two parameters. More speciﬁcally, when the edges of their caps started to become ﬂat or slightly uprolled, it was an indication of their ﬁnal stage of growth [31]. Figure 1 shows some examples of mushrooms belonging to the three different stages. Many of the collected photos in the greenhouse contained multiple mushrooms, making the annotation complex and the recognition even more challenging, especially when the mushrooms were at the ﬁnal stage and many of them were overlapping. Figure 2 presents two of those photos as an example. Figure 2. Pictures from the greenhouse with multiple mushrooms. Finally, 4271 mushrooms were annotated and classiﬁed into three different classes (Stage1, Stage2, Stage3) as described before. Moreover, 1130 of them belonged to the ﬁrst growing stage, 1845 to the second growing stage, and 1296 of them to the third growing stage. In addition, the 1128 annotated photos were divided into 784 for the training dataset and 344 for the validation dataset. For the second part of our research, we collected a different set of photos. In particular, the data acquisition for the growing rate of mushrooms was made in 33 different mushroom substrate grow bags. We captured a photo for each one on seven different days between 16 August 2022 and 24 August 2022. Finally, we collected 231 different photos in total. The distance between the camera and the substrate grow bag was one meter above. Figure 3 shows an example of three captured photos. We did not annotate these photos, but we used the trained models from Detectron2 to indicate the size of each mushroom. In our evaluation for mushroom growth monitoring, we used only the mushrooms that were on top of the substrate grow bag. For training Detectron2 for instance segmentation, we annotated another two datasets. The ﬁrst one contained annotations for substrate grow bags, and the second one contained annotations for mushrooms. The annotation process for the image segmentation is a time- consuming task since we had to annotate precisely the objects with polygons. For that reason, we chose to annotate only photos that contained a small number of mushrooms to simplify the annotation. Finally, we annotated 453 mushrooms and divided them into 358 for the training dataset and 95 for the validation dataset. In addition, for the detection of substrate grow bags, we annotated 200 photos and divided them into 150 for the training dataset and 50 for the validation dataset. For mushroom detection and classiﬁcation in the three different classes as described previously, we used YOLOv5. First, we trained our models with the default hyperparame- ters using the pretrained models YOLOv5s and YOLOv5l with an image size of 640 pixels and batch sizes of 2, 4, and 8. To achieve better results, we used the provided method integrated with YOLOv5 for hyperparameter optimisation called hyperparameter evolution. It is a genetic algorithm to ﬁnd the best set of hyperparameters for the speciﬁc dataset. The evolving procedure starts from the default hyperparameters or other user-deﬁned values, if available, and tries to improve a ﬁtness function in every loop. The default ﬁtness function is a weighted combination of mAP@0.5 with 10% contribution and mAP@0.5:0.95 with 90% contribution. In every evolving loop, the primary genetic operator is the mutation. The proposed combination for the mutation uses 80% probability and a 0.04 variance to calculate the next combination of hyperparameters based on the best parents from previous generations. We used the hyperparameter evolution approach for the two pretrained models, YOLOv5s and YOLOv5l, and trained them for 600 generations with an image size of 640 pixels and a batch size of 4. Table 1 shows the predicted sets of hyperparameters for each pretrained model. Table 1. Optimised hyperparameters after evolving for YOLOv5s and YOLOv5l. Figure 4 shows a graphical representation for YOLOv5l with each hyperparameter displayed in a different subplot. Each subplot presents all values from all generations for the speciﬁc hyperparameter. The horizontal axis corresponds to the value of the hyperparameter, and the vertical value corresponds to the calculated ﬁtness. The yellow areas indicate a high concentration of values. Subplots with vertical distributions indicate that the speciﬁc hyperparameter was disabled and did not mutate. Figure 4. Graphical representation for the calculated values of all hyperparameters, after evolving for 600 generations for YOLOv5l. The mAP@0.5 metric corresponds to the average precision over a threshold of 0.5 for the object detection. In addition, the mAP@0.5:0.95 denotes the average precision with a threshold between 0.5 and 0.95. The precision metric indicates the correctly identiﬁed trees divided by the total number of detected trees and is given by Equation (1). The recall metric indicates the falsely identi- ﬁed trees divided by the total number of actual existing trees and is given by Equation (2). where TP (true positive) is the number of correctly detected objects in the class, FP (false positive) is the number of falsely detected objects in the speciﬁc class, and FN (false negative) is the number of objects that are not detected in the speciﬁc class. For mushroom growth monitoring, we used two different stages for the object detec- tion. The ﬁrst one was responsible for detecting the substrate grow bag, and the second one was responsible for detecting the mushrooms in a photo. Figure 5 illustrates the main components of the architecture of the proposed method. More speciﬁcally, in the stage of the substrate grow bag detection, we use the trained model with Detectron2. We only obtained the bounding box of the detected substrate grow bag and not the mask. After the detection of the substrate grow bag, we used the corresponding bounding box and divided it into six equal rectangles (2 × 3). Figure 6a illustrates the bounding box (green) and the lines (red) that divide the bounding box in order to distinguish the detected mushrooms. Moreover, we used the trained model with Detectron2 to detect the mushrooms in the photo and return the corresponding masks. Figure 6b displays the detected mush- rooms. Next, we performed a resize operation on the bounding box and the masks to achieve normalisation for calculating the size in all photos of the same substrate grow bag. Furthermore, the detected mushrooms were distinguished in one of the rectangles with the following rules. A mushroom should have more than 50% in the speciﬁc rectangle. Only one mushroom can belong in one rectangle, and each mushroom belongs only in one rectangle. In any case, the selected mushroom was the one that covered the most area in the rectangle. Finally, Figure 6c illustrates the divided bounding box with the corresponding masks of all detected masks. The size of the provided mask could indicate the size of the mushroom. After distinguishing each mushroom, we calculated the coverage based on the pixels of the provided mask and used it for mushroom growth monitoring. \\n In the following subsections, we evaluate the results of the mushroom classiﬁcation for different growth stages. Next, we provide the results of mushroom monitoring for their growth rate. In this subsection, we compared different conﬁgurations of YOLOv5 for mushroom detection and classiﬁcation in three classes, as described previously. We used the metrics mAP@0.5, mAP@0.5:0.95, precision, recall, and F1-score to compare the accuracy of each trained model. Table 2 shows a comparison of all conﬁgurations. More speciﬁcally, we use the pretrained models YOLOv5s and YOLOv5l with the default (D) conﬁguration for the hyperparameters and with the conﬁguration derived from evolution (E). Moreover, our conﬁgurations used an image size of 640 pixels and three different values for the batch size (2, 4, 8). Table 2. Evaluation results for mAP@0.5 and mAP@0.5:0.95 for different hyperparameter conﬁgura- tions (HC), for default values (D) and optimised values with evolution (E). It seems that there was not much difference in all conﬁgurations. Based on mAP@50 and mAP@50:95, conﬁgurations with hyperparameters derived from evolution showed a slightly better performance. Moreover, conﬁgurations based on YOLOv5l returned a slightly better performance. The model based on YOLOv5l with hyperparameters de- rived from evolution and with a batch size of eight achieved the best performance. More speciﬁcally, it reached an mAP@0.5 of 0.79353 and an mAP@0.5:0.95 of 0.54582. Another type of evaluation was based on the F1-Score metric. Table 3 shows the comparison of all conﬁgurations based on the best occurrence of the F1-Score. Better performance on this metric was achieved by the same conﬁguration with YOLOv5l, hy- perparameters derived from evolution, and a batch size of eight, as it reached an F1-Score of 76.65%. Table 3. Evaluation results for the F1-Score for different hyperparameter conﬁgurations (HC), for default values (D) and optimised values with evolution (E). Figure 7 shows the curve of the F1-Score for the best conﬁguration. The horizontal axis corresponds to the conﬁdence of the detected mushrooms, while the vertical axis corresponds to the F1-score. In this example, the maximum F1-score had a conﬁdence of 0.566. In addition, the graph shows that the class “Stage1” had a better accuracy. This was expected since all photos with mushrooms belonging to “Stage1” were simpler, as mushrooms were small and not overlapping. To notice the detection accuracy for each class more clearly, Figure 8 presents the confusion matrix for the same conﬁguration. It shows that class “Stage1” had an accuracy of 82%, class “Stage2” had an accuracy of 71%, and “Stage3” had an accuracy of 70%. Although this does not seem perfect, we can say that it was good enough depending on the complexity of the photos with mushrooms in “Stage2” and “Stage3”. Figure 9 shows an example of the detection where only mushrooms from classes Stage2 and Stage3 exist. The green bounding box indicates that the mushroom is ready to harvest (Stage3). The blue bounding box indicates that the mushroom is not yet ready to harvest (Stage2). We marked in magenta the mushrooms that were detected in the wrong class. Moreover, we marked in red the mushrooms that were not detected. It seems from the photo that the mushrooms in the foreground were detected in the right class, while some errors occurred in mushrooms that were in the background or partially displayed. After the procedure described previously, we obtained 575 different masks from 114 different mushrooms in the greenhouse. All masks were distinguished for different mushrooms depending on the number of the substrate grow bag and the position on it. When the mask corresponded to the ﬁrst day of appearance of each mushroom, it was marked as day one. The corresponding masks of the next days of the same mushroom were numbered as well in the same way. Figure 10 shows the growth rate of four different mushrooms. Each mushroom is indicated with a different colour. The horizontal axis of the graph contains the number of the growth day, starting from number one for the ﬁrst day of the mushroom’s appearance. The vertical axis indicates the size of the mushroom counted in pixels. Moreover, we calculated the maximum size of each mushroom based on all photos before it was harvested. We set its size as 100%, and we calculated the size of the same mushroom for all other days accordingly. Figure 11 shows the growth rate of the same mushrooms but as a percentage of the maximum size of each mushroom. Both graphs show that the three mushrooms (M1, M2, M3) followed an almost linear growth rate. The fourth mushroom (M4) also followed a linear growth rate for the ﬁrst four days, but from the ﬁfth day, it seemed not to change in size. In fact, on the ﬁfth day, that speciﬁc mushroom should have been harvested, but it was accidentally forgotten. Figure 12 shows the captured photos of this mushroom (M4), from which it seemed that the mushroom was ready to harvest on day ﬁve. Figure 12. Mushroom growth example photos. We manually evaluated the harvesting status of each mushroom and decided that 17 mushrooms could be harvested earlier, which represented 14.04% of the mushrooms. The average difference between the size of the masks from the day identiﬁed as ready to harvest and the previous day was 5.34%. Thus, if we set it as a threshold in our method to provide an alert any time that the difference in the mushroom size was below it, we could inform the farmer of the harvest time. This would lead to an improvement in the quality of the harvested mushrooms as we would avoid overripe mushrooms. In addition, our evaluation showed that the average days for the growth of mushrooms was 5.22. Figure 13 displays the graph with all mushrooms detected for all days. All values are displayed in percentage of the maximum size of the speciﬁc mushroom. The blue marks indicate the detected mushrooms that were not ready to harvest, while the red marks indicate mushrooms that should have been harvested at least one day before. \\n Our ﬁrst approach for mushroom detection with YOLOv5 could be a valuable part of a decision support system. For example, it may be useful in tasks such as yield prediction or product improvement by indicating those mushrooms that are ready for harvest. Overall, the accuracy of the trained model was considered acceptable regarding the complexity of the photos of the dataset. However, our experience shows that some changes could improve the results. A more precisely selected dataset should improve the detection accuracy of different classes. For example, photos with fewer mushrooms and only in the foreground will give more accurate results. As we discussed in Figure 9, the main drawback in detection accuracy is that many mushrooms are partially visible in the photos, and many of them are in the background, which makes it difﬁcult to detect and classify because their features are not clear. Moreover, a proposed methodology to overcome these obstacles could be to exclude mushrooms that are partially displayed in the picture and to focus only on those clearly displayed. Furthermore, the 600 steps we used for the hyperparameter evolution in this research may not be good enough, and more steps will result in a better detection accuracy. This would lead to better combinations of the hyperparameters, but the hyperparameter evolu- tion procedure is a very time-consuming task, even for a professional graphic processing unit. A better approach to enhance the model’s accuracy is to modify the YOLOv5 algorithm. Thus, the trained model would include the speciﬁc characteristics of the different classes. Our second approach for mushroom growth monitoring could also be helpful for a decision support system to inform farmers about the harvesting time. As a result, an improvement in product quality is expected since mushrooms will be collected at the right time. Furthermore, the precisely detected mask of the mushrooms may support other potential applications. For example, it could be a useful part of a robotic mechanism in order to detect, decide, and collect only those mushrooms that are ready to harvest without damaging them. The method of mushroom growth monitoring with Detectron2 also has some limi- tations. Our method focused only on mushrooms that were on top of the substrate bag, which was placed horizontally in the greenhouse. In fact, only one-third of the mushrooms could be observed with this method since there were also substrate bags positioned verti- cally below the substrate bag we used. In addition, in many cases, the substrate bags are densely placed in the greenhouse, and sometimes, the farmers use more than one layer of substrate grow bags in the greenhouse, making it difﬁcult to capture photos even from those positioned horizontally. Furthermore, getting only one image per day has the disadvantage that the growing days may not be counted precisely in some mushrooms. For example, some mushrooms may appear a few hours after a picture was taken, resulting in a day count less than the actual growth duration. Moreover, some mushrooms may have been harvested just before a picture was taken, also resulting in reduced count days. Therefore, capturing photos of the substrate bags more often could give better results and inform more precisely about the growing rate of the mushroom and the harvesting time. Finally, the results of the mushroom detection are not always precise. In fact, the proposed method has two minor drawbacks. First, some of the mushrooms that were too small could not be detected. In addition, the masks returned from Detectron2 were not always accurate. We believe that both these drawbacks do not have a large impact on the proposed methodology. However, if we need to improve them, we could use a larger dataset with more annotated mushrooms to get better results. \\n Object detection and instance segmentation have multiple applications in various domains. In smart farming, it is mainly used in fruit detection, weed detection, or pest detection. In this paper, we evaluated the effectiveness of YOLOv5 and Detectron2 in mushroom detection in a greenhouse. Firstly, we evaluated YOLOv5 on its effectiveness in classifying mushrooms in three growth stages. The results showed that even in complex environments such as a greenhouse with Pleurotus mushrooms, it was possible to identify mushrooms that were ready to harvest. The evaluation results on mushroom detection and classiﬁcation in three different growing stages gave an F1-score of up to 76.5%, and speciﬁcally for the ﬁnal growing stage, an accuracy of up to 70%. Secondly, we proposed a method for mushroom growth monitoring. For that purpose, we used two trained models with Detectron2. The ﬁrst one detected the substrate grow bag, and the second detected the mushrooms and returned the corresponding masks. Experi- mental results showed that the growth rate of Pleurotus mushrooms was linear. Moreover, this procedure made it possible to detect when mushrooms reached their maximum size and were ready to harvest. The evaluation results showed that it was possible to make de- cisions and improve harvesting time for up to 14.04% of the mushrooms in the greenhouse. In addition, the results showed that, on average, Pleurotus mushrooms needed 5.22 days to reach maximum size. Moreover, the proposed methods are suitable to be part of a decision support system to inform farmers about the status of their cultivation. In addition, a future potential application would be a robotic mechanism able to detect and recognise the growth stage of mushrooms before harvesting them. Author Contributions: Conceptualization, V.M. and P.S.; methodology, V.M. and G.K.; software, V.M.; validation, G.K. and P.S.; formal analysis, V.M.; investigation, V.M.; resources, V.M. and N.M.; data curation, V.M. and G.K.; writing—original draft preparation, V.M.; writing—review and editing, S.B. and I.M.; visualization, V.M.; supervision, P.S.; project administration, P.S.; funding acquisition, P.S. All authors have read and agreed to the published version of the manuscript. Funding: This research was cofunded by the European Regional Development Fund of the European Union and Greek national funds through the Operational Program Western Macedonia 2014–2020, under the call “Collaborative and networking actions between research institutions, educational institutions and companies in priority areas of the strategic smart specialization plan of the region”, project “Smart Mushroom fARming with internet of Things—SMART”, project code: DMR-0016521. Data Availability Statement: The data presented in this study are available on request from the corresponding author. The data are not publicly available due to restrictions on privacy. '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_parts_without_chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding(text: str, chunk_word_size: int = 288, chunk_overlap: int = 0) -> List[Dict[str, Any]]:\n",
    "    words = text.split(\" \")\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for idx, i in enumerate(range(0, len(words), chunk_word_size - chunk_overlap)):\n",
    "        chunk = words[i:i + chunk_word_size]\n",
    "        if chunk:\n",
    "            out.append(\n",
    "                {\n",
    "                    \"index\": idx,\n",
    "                    \"num_words\": len(chunk),\n",
    "                    \"text\": \" \".join(chunk),\n",
    "                }\n",
    "            )            \n",
    "    return out\n",
    "\n",
    "g = sliding(combined_parts_without_chapters, chunk_word_size=288, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97072ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overlapping_chunks(text, MAX_CHUNK_SIZE=365, OVERLAP_MAX_SIZE=73):\n",
    "    # List to hold the final output\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    # Split the text using \". \" and \".<Capital letter>\" as delimiters\n",
    "    pattern = r'(?<=\\.)\\s+|(?<=\\.)(?=[A-Z])'\n",
    "    sentences = [s for s in re.split(pattern, text) if s]\n",
    "    # Initialize variables for chunking\n",
    "    chunks = []\n",
    "    chunk_words = 0\n",
    "    for idx,sentence in enumerate(sentences):\n",
    "        # Split the sentence into words\n",
    "        words = sentence.split(\" \")\n",
    "        # Start the first chunk with the first sentence\n",
    "        if chunk_words == 0:\n",
    "            chunk_sentence_ids = [idx]\n",
    "            chunk_words = len(words)\n",
    "        # If the current chunk plus the new sentence is within the max size, add it\n",
    "        elif chunk_words + len(words) < MAX_CHUNK_SIZE:\n",
    "            chunk_sentence_ids.append(idx)\n",
    "            chunk_words += len(words)\n",
    "        # If adding the new sentence exceeds the max size, create a new chunk\n",
    "        else:\n",
    "            chunks.append(chunk_sentence_ids)\n",
    "            chunk_sentence_ids = []\n",
    "            overlapping_words = 0\n",
    "            # Check how many sentences can be added from the end of the current chunk to the new chunk for overlapping\n",
    "            for y in reversed(chunks[-1]):\n",
    "                overlapping_words += len(sentences[y].split(\" \"))\n",
    "                if overlapping_words < OVERLAP_MAX_SIZE:\n",
    "                    chunk_sentence_ids.append(y)\n",
    "                else:\n",
    "                    break\n",
    "            chunk_sentence_ids.reverse()\n",
    "            chunk_sentence_ids.append(idx)\n",
    "            chunk_words = len(words)\n",
    "\n",
    "    # Convert from sentence indices to actual text chunks\n",
    "    text_chunks = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        text_chunks.append(\" \".join([sentences[i] for i in chunk]))\n",
    "        out.append(\n",
    "            {\n",
    "                \"index\": idx,\n",
    "                \"num_words\": len(text_chunks[-1].split(\" \")),\n",
    "                \"text\": text_chunks[-1],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "b = create_overlapping_chunks(combined_parts_without_chapters, MAX_CHUNK_SIZE=365, OVERLAP_MAX_SIZE=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8957c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
